{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bark Beetles: Predicting the Plague - Data Aggreation\n",
    "\n",
    "#### Modeling the spruce bark beetle infestation for given spatial administrative units within Saxony and for distictive time intervals on the basis of the infestation development and the weather pattern up to the time of the forecast\n",
    "\n",
    "**by**\n",
    "Yannic Holländer\n",
    "\n",
    "**Abstract**\n",
    "This notebook encompasses the merging of various data sources. We create a single dataset containing all relevant information on the bark beetle infestation in Saxony. Subsequent notebooks use this dataset for the exploratory data analysis and model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview of available data\n",
    "\n",
    "For this project there are four main data sources. These data sources are:\n",
    "\n",
    "1. **The infestation history**\n",
    "    * contains all observations for the amount of accrued infested wood (target variable, in solid m$^3$)\n",
    "    * also contains the timeframe for these observations, the respective forestry district, the type of forest (sepeartion by private/state owned) and the amount of disposed wood in this time period\n",
    "    * data supplied by *Staatsbetrieb Sachsenforst*\n",
    "\n",
    "\n",
    "2. **Information on the forestry districts**\n",
    "    * contrary to previous approaches, we are predicting the amount of infested wood not for the whole state of Saxony, but for given spatial administrative units - forestry districts - of which there currently are 53\n",
    "    * contains the geodata/polygons for these districts\n",
    "    * also for every district, contains the area covered by forest, separated by private/state owned forest as well as endangered and safe forest area (endangered are only sections that consist predominantely of adult spruce trees)\n",
    "    * the forestry district borders changed slightly in 2013/2014, we have the shape of the old districts as well as the new districts\n",
    "    * data supplied by *Staatsbetrieb Sachsenforst*\n",
    "\n",
    "\n",
    "3. **Meteorological raster data**\n",
    "    * contain certain climatic parameters such as the mean temperature, humidity, wind speeds, global irridiation etc. (15 variables total)\n",
    "    * one raster file for every variable and month/day of the covered time period (from 2005 up to February 2020)\n",
    "    * 1000mx1000m raster\n",
    "    * supplied by ReKIS (*Regionales Klima-Informationssystem Sachsen, Sachsen-Anhalt und Thüringen*, https://rekis.hydro.tu-dresden.de/)\n",
    "    \n",
    "    \n",
    "4. **Information on abiotic damages**\n",
    "    * covers windfall and demolition wood, damages from drought, storm, ice, snow etc.\n",
    "    * gathered semi-anually (april and september)\n",
    "    * similar variables to infestation_history\n",
    "    * data supplied by *Staatsbetrieb Sachsenforst*\n",
    "    \n",
    "To make sense of the data we will have to aggregate this information into a single dataframe that can be used for an EDA and the modeling process. \n",
    "\n",
    "\n",
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import rasterio \n",
    "import rasterio.plot\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# diplay all columns of a dataframe\n",
    "pd.options.display.max_columns= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in the infestation history dataset, which was supplied as a Microsoft Excel file. We use it as a skeleton on which information from the other sources is added on. This way we build our dataset on top of central observations of the target variable. It has the following columns:\n",
    "\n",
    "* **county_acronym** - A shorthand of the respective county. The 53 forestry districts form the 13 counties of the free state of Saxony. \n",
    "* **county_nr** - The number of the respective county. Every county has a unique double-digit number.\n",
    "* **fdist_nr** - Denotes the forestry district within the county. In combination with county_nr makes up the unique identifier for forestry districts (fdist_id)\n",
    "* **fdist_id** - Unique identifier for the forestry district. Has four digits. The first two digits are made up of county_nr and the last two are made up of fdist_nr.\n",
    "* **year** - Year of the observation. Ranges from 2006 until 2020. Last observation is from September 2020\n",
    "* **timeframe** - The timeframe of the observation within the year. The data is gathered monthly from April till September and quarterly from October till March.\n",
    "* **forest_ownership** - A binary categorial variable that distinguishes between state owned forest (SW) and private/corporate forest (NSW)\n",
    "* **infested_wood** - Target variable. The amount of accrued infested wood in solid cubic metres. \n",
    "* **disposed_wood** - Infested wood that was disposed, i.e. the infested trees were cut down and removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_acronym</th>\n",
       "      <th>county_nr</th>\n",
       "      <th>fdist_nr</th>\n",
       "      <th>fdist_id</th>\n",
       "      <th>year</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>forest_ownership</th>\n",
       "      <th>infested_wood</th>\n",
       "      <th>disposed_wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>06 Juni</td>\n",
       "      <td>SW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>08 August</td>\n",
       "      <td>SW</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2008</td>\n",
       "      <td>04 April</td>\n",
       "      <td>SW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2008</td>\n",
       "      <td>06 Juni</td>\n",
       "      <td>SW</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  county_acronym  county_nr  fdist_nr  fdist_id  year            timeframe  \\\n",
       "0             BZ         25         1      2501  2007              06 Juni   \n",
       "1             BZ         25         1      2501  2007            08 August   \n",
       "2             BZ         25         1      2501  2007  10 Oktober-Dezember   \n",
       "3             BZ         25         1      2501  2008             04 April   \n",
       "4             BZ         25         1      2501  2008              06 Juni   \n",
       "\n",
       "  forest_ownership  infested_wood  disposed_wood  \n",
       "0               SW            5.0            0.0  \n",
       "1               SW           12.0           12.0  \n",
       "2               SW            2.0            0.0  \n",
       "3               SW            1.0            0.0  \n",
       "4               SW            2.0            0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the infestation history data\n",
    "infestation_history = pd.read_excel(\n",
    "    r'data_raw/ML_BDR_20201019.xlsx', \n",
    "    names=['county_acronym', \n",
    "           'county_nr', \n",
    "           'fdist_nr', \n",
    "           'fdist_id',\n",
    "           'year', \n",
    "           'timeframe', \n",
    "           'forest_ownership', \n",
    "           'infested_wood', \n",
    "           'disposed_wood'])\n",
    "\n",
    "# display first few rows of the dataframe\n",
    "infestation_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Forestry Districts\n",
    "## 3.1 Preparing the infestation history dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fdist_id column of our infestation_history dataframe contains a unique identifier for the forstry districts. The first two digits indicate the county (*Landkreis*) and the last two digits indicate the number of the forestry district in this county. These digits correspond to the county_nr and fdist_nr columns respectively.\n",
    "\n",
    "In some forestry districts the district number (last two digits) begins with a leading nine instead of a leading zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 1101,\n",
       "       1201, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2191, 2192, 2193,\n",
       "       2194, 2195, 2196, 2197, 2198, 2201, 2202, 2203, 2204, 2601, 2602,\n",
       "       2603, 2604, 2605, 2606, 2691, 2901, 2902, 2701, 2702, 2703, 2704,\n",
       "       2791, 2792, 2793, 2801, 2802, 2803, 2804, 2805, 3001, 3002, 3003,\n",
       "       2301, 2302, 2303, 2304, 2305, 2306, 2401, 2402], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display all forestry district numbers\n",
    "infestation_history['fdist_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the observation timeframe, some of the forestry districts (counties *Erzgebirgskreis* and *Meißen*) underwent a restructuring process. A leading nine instead of a leading zero signifies that the border of the district was different than it is today. According to *Sachsenforst* these changes happened in July 2013 for the county *Meißen* (fdist_id 27xx) and in September 2014 for *Erzgebirgskreis* (fdist_id 21xx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fdist_id\n",
       "2191    2014\n",
       "2192    2014\n",
       "2193    2014\n",
       "2194    2014\n",
       "2195    2014\n",
       "2196    2014\n",
       "2197    2014\n",
       "2198    2014\n",
       "2691    2020\n",
       "2791    2013\n",
       "2792    2013\n",
       "2793    2013\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the max years for districts with a fdist_nr with leading nine\n",
    "infestation_history[\n",
    "    infestation_history['fdist_nr'] >= 90\n",
    "].groupby('fdist_id')['year'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two parts of 'fdist_id' also appear in the 'county_nr' and 'fdist_nr' columns seperately. Thus they are redundant. We check if the information in these three columns really is the same for every observation. If that is the case we drop 'county_nr' and 'fdist_nr':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# county_nr as as a string\n",
    "county_nr = infestation_history[\n",
    "    'county_nr'\n",
    "].astype(str) \n",
    "# fdist_nr as a string (pad with 0)\n",
    "fdist_nr = infestation_history[\n",
    "    'fdist_nr'\n",
    "].astype(str).map(lambda x: x.zfill(2)) \n",
    "\n",
    "# concatenate these strings and check if they are identical to the 'fdist_id' column at every observation\n",
    "(county_nr + fdist_nr == infestation_history['fdist_id'].astype(str)).all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our comparison states that the columns are redunant. We drop county_nr and fdist_nr in favor of fdist_id which combines them into a single, four-digit identifier. We also drop the county_acronym column which only states the county initials of the county_nr. A full association of the forestry district and county names is found in the forestry district geodata which we will load in later. For now the fdist_id countains all information we need in this dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'county_nr' and 'fdist_nr' columns \n",
    "# because the information is also found in 'fdist_id'\n",
    "infestation_history.drop(\n",
    "    [\n",
    "        'county_nr', \n",
    "        'fdist_nr', \n",
    "        'county_acronym'\n",
    "    ], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue examining the cases with leading 9s. The *Stadtwald Zittau* (fdist_id 2691) is a special case among those special cases. From the fdist_id and the maximum year of occurence in the data we can already conclude that it is not in either of the restructured counties. According to Sachsenforst the correct procedure for this fdist_id is to just add the corresponding observations to the forestry district *Zittau* (fdist_id 2601)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate fdist_id 2691 to fdist_id 2601 and add the observations\n",
    "\n",
    "# in column 'fdist_id' change all occurrences of 2691 to 2601\n",
    "infestation_history['fdist_id'] = infestation_history[\n",
    "    'fdist_id'\n",
    "].replace(2691, 2601)\n",
    "\n",
    "# aggregate the values \n",
    "# by summing them together for the 'infested_wood' column\n",
    "# but only if every other column value is the same (same observation)\n",
    "infestation_history['infested_wood'] = infestation_history.groupby(\n",
    "    [\n",
    "        'fdist_id', \n",
    "        'year', \n",
    "        'timeframe', \n",
    "        'forest_ownership'\n",
    "    ]\n",
    ")['infested_wood'].transform('sum')\n",
    "\n",
    "#  do the same for the 'disposed_wood' column\n",
    "infestation_history['disposed_wood'] = infestation_history.groupby(\n",
    "    [\n",
    "        'fdist_id', \n",
    "        'year', \n",
    "        'timeframe', \n",
    "        'forest_ownership'\n",
    "    ]\n",
    ")['disposed_wood'].transform('sum')\n",
    "\n",
    "# now the values are aggregated correctly but duplicated \n",
    "# (since we did't change the shape of the dataframe)\n",
    "# drop the duplicated rows that were just created\n",
    "infestation_history.drop_duplicates(inplace=True)\n",
    "\n",
    "# reset index\n",
    "infestation_history.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Loading and formating forestry district information\n",
    "\n",
    "For the remaining forestry districts we need to distinguish between the old borders and the new ones. Sachsenforst supplied us with two shape files, one with the current district borders for all forestry districts and one with only old borders of districts that changed in some way. The districts in the old shapefile are not yet formated in the same way as our infestation_history dataframe. We have to change the 'fdist_id' numbers for the abolished districts so they match the format with the leading 9s. Currently they still have leading zeros. After that we merge both geodataframes.\n",
    "\n",
    "Every row corresponds to one forestry district. The column names are as follows:\n",
    "* **county_name** - The name of the county this forestry district is located in.\n",
    "* **fdist_name** - The name of the forestry district.\n",
    "* **fdist_id** - Unique identifier for the forestry district. Has four digits. The first two digits are made up of county_nr and the last two are made up of fdist_nr. For the file with the new borders, same as the fdist_id in the infestation_history dataframe. For the file with the old borders, the third digit still has to be changed to a nine to match the fdist_id in the infestation_history dataframe.\n",
    "* **area_nse** - Forest area (in ha) that is not state owned (private/corporate forest) and endangered by the spruce bark beetle. Endangered forest is forest with a spruce ratio > 10% and a tree height >= 20 m.\n",
    "* **area_nsne** - Forest area (in ha) that is not state owned (private/corporate forest) and not endangered by the spruce bark beetle.\n",
    "* **area_se** - Forest area (in ha) that is state owned and endangered by the spruce bark beetle. Endangered forest is forest with a spruce ratio > 10% and a tree height >= 20 m.\n",
    "* **area_sne** - Forest area (in ha) that is state owned (private/corporate forest) and not endangered by the spruce bark beetle.\n",
    "* **geometry** - The forestry district border as polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>fdist_name</th>\n",
       "      <th>fdist_id</th>\n",
       "      <th>area_nse</th>\n",
       "      <th>area_nsne</th>\n",
       "      <th>area_se</th>\n",
       "      <th>area_sne</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mittelsachsen</td>\n",
       "      <td>Reinsberg</td>\n",
       "      <td>2203</td>\n",
       "      <td>1597.32</td>\n",
       "      <td>3274.630917</td>\n",
       "      <td>2706.18</td>\n",
       "      <td>2133.910411</td>\n",
       "      <td>POLYGON ((386902.476 5656907.025, 386910.595 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mittelsachsen</td>\n",
       "      <td>Geringswalde</td>\n",
       "      <td>2201</td>\n",
       "      <td>841.61</td>\n",
       "      <td>3508.605810</td>\n",
       "      <td>196.15</td>\n",
       "      <td>1453.972847</td>\n",
       "      <td>POLYGON ((332902.962 5650328.573, 332905.989 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leipzig</td>\n",
       "      <td>Leipziger Land</td>\n",
       "      <td>2902</td>\n",
       "      <td>401.71</td>\n",
       "      <td>8199.853850</td>\n",
       "      <td>615.51</td>\n",
       "      <td>5314.476829</td>\n",
       "      <td>POLYGON ((332897.160 5650325.466, 332893.592 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mittelsachsen</td>\n",
       "      <td>Striegistal</td>\n",
       "      <td>2202</td>\n",
       "      <td>954.18</td>\n",
       "      <td>3156.650864</td>\n",
       "      <td>1147.04</td>\n",
       "      <td>1844.186239</td>\n",
       "      <td>MULTIPOLYGON (((377509.195 5657427.330, 377569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>Süd</td>\n",
       "      <td>2703</td>\n",
       "      <td>392.75</td>\n",
       "      <td>4365.001441</td>\n",
       "      <td>381.91</td>\n",
       "      <td>1973.920712</td>\n",
       "      <td>POLYGON ((377329.166 5657157.286, 377285.838 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     county_name      fdist_name fdist_id  area_nse    area_nsne  area_se  \\\n",
       "0  Mittelsachsen       Reinsberg     2203   1597.32  3274.630917  2706.18   \n",
       "1  Mittelsachsen    Geringswalde     2201    841.61  3508.605810   196.15   \n",
       "2        Leipzig  Leipziger Land     2902    401.71  8199.853850   615.51   \n",
       "3  Mittelsachsen     Striegistal     2202    954.18  3156.650864  1147.04   \n",
       "4         Meißen             Süd     2703    392.75  4365.001441   381.91   \n",
       "\n",
       "      area_sne                                           geometry  \n",
       "0  2133.910411  POLYGON ((386902.476 5656907.025, 386910.595 5...  \n",
       "1  1453.972847  POLYGON ((332902.962 5650328.573, 332905.989 5...  \n",
       "2  5314.476829  POLYGON ((332897.160 5650325.466, 332893.592 5...  \n",
       "3  1844.186239  MULTIPOLYGON (((377509.195 5657427.330, 377569...  \n",
       "4  1973.920712  POLYGON ((377329.166 5657157.286, 377285.838 5...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the first shape file as a geopandas geodataframe\n",
    "districts_new = gpd.read_file(\n",
    "    r'data_raw/shape/ufb_rev_wald_teil.shp', \n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# column names\n",
    "districts_new.columns=[\n",
    "    'county_name',\n",
    "    'fdist_name', \n",
    "    'fdist_id', \n",
    "    'area_nse', \n",
    "    'area_nsne', \n",
    "    'area_se', \n",
    "    'area_sne', \n",
    "    'geometry'\n",
    "]\n",
    "\n",
    "# display first few rows of the dataframe\n",
    "districts_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>fdist_name</th>\n",
       "      <th>fdist_id</th>\n",
       "      <th>area_nse</th>\n",
       "      <th>area_nsne</th>\n",
       "      <th>area_se</th>\n",
       "      <th>area_sne</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>Nord</td>\n",
       "      <td>2703</td>\n",
       "      <td>143.31</td>\n",
       "      <td>5780.407594</td>\n",
       "      <td>1.09</td>\n",
       "      <td>768.093453</td>\n",
       "      <td>POLYGON ((418952.942 5692288.782, 418909.147 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>West</td>\n",
       "      <td>2701</td>\n",
       "      <td>22.80</td>\n",
       "      <td>4255.041515</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3650.063576</td>\n",
       "      <td>POLYGON ((389635.997 5699901.234, 389648.747 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>Süd</td>\n",
       "      <td>2702</td>\n",
       "      <td>411.13</td>\n",
       "      <td>4543.837549</td>\n",
       "      <td>381.83</td>\n",
       "      <td>1975.417673</td>\n",
       "      <td>POLYGON ((378695.051 5678837.912, 378676.082 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erzgebirgskreis</td>\n",
       "      <td>Annaberg</td>\n",
       "      <td>2105</td>\n",
       "      <td>2408.13</td>\n",
       "      <td>1882.699804</td>\n",
       "      <td>6142.78</td>\n",
       "      <td>2810.089861</td>\n",
       "      <td>MULTIPOLYGON (((366499.454 5606840.063, 366468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Erzgebirgskreis</td>\n",
       "      <td>Eibenstock</td>\n",
       "      <td>2101</td>\n",
       "      <td>726.57</td>\n",
       "      <td>1754.566081</td>\n",
       "      <td>10922.47</td>\n",
       "      <td>3573.808856</td>\n",
       "      <td>POLYGON ((333355.788 5609845.954, 333373.979 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       county_name  fdist_name fdist_id  area_nse    area_nsne   area_se  \\\n",
       "0           Meißen        Nord     2703    143.31  5780.407594      1.09   \n",
       "1           Meißen        West     2701     22.80  4255.041515      3.93   \n",
       "2           Meißen         Süd     2702    411.13  4543.837549    381.83   \n",
       "3  Erzgebirgskreis    Annaberg     2105   2408.13  1882.699804   6142.78   \n",
       "4  Erzgebirgskreis  Eibenstock     2101    726.57  1754.566081  10922.47   \n",
       "\n",
       "      area_sne                                           geometry  \n",
       "0   768.093453  POLYGON ((418952.942 5692288.782, 418909.147 5...  \n",
       "1  3650.063576  POLYGON ((389635.997 5699901.234, 389648.747 5...  \n",
       "2  1975.417673  POLYGON ((378695.051 5678837.912, 378676.082 5...  \n",
       "3  2810.089861  MULTIPOLYGON (((366499.454 5606840.063, 366468...  \n",
       "4  3573.808856  POLYGON ((333355.788 5609845.954, 333373.979 5...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the second shape file as a geopandas geodataframe\n",
    "districts_old = gpd.read_file(\n",
    "    r'data_raw/shape/ufb_rev_vorUmstrukturierungen.shp', \n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# column names\n",
    "districts_old.columns=[\n",
    "    'county_name', \n",
    "    'fdist_name', \n",
    "    'fdist_id', \n",
    "    'area_nse', \n",
    "    'area_nsne', \n",
    "    'area_se', \n",
    "    'area_sne', \n",
    "    'geometry'\n",
    "]\n",
    "# display first few rows of the dataframe\n",
    "districts_old.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the districts_old geodataframe so it matches the notation in the infestation_history dataframe. This is done by changing the third digit from a zero to a nine. We also change the fdist_id in both geodataframes from a string to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the leading 9 notation for abolished forestry districts\n",
    "# add 90 to every 'fdist_id' in the districts_old dataframe \n",
    "districts_old['fdist_id'] = districts_old['fdist_id'].astype(int) + 90\n",
    "\n",
    "# also change 'fdist_id' of districts_new to type int\n",
    "districts_new['fdist_id'] = districts_new['fdist_id'].astype(int)\n",
    "\n",
    "# merge the geodataframes\n",
    "districts = pd.merge(districts_new, districts_old, how ='outer') \n",
    "\n",
    "# shape should be 64x8 now (53 new districts + 11 old districts)\n",
    "districts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make one more modification and the change some forestry district names slightly, to make them unambiguous. Currently, the counties *Meißen* and *Zwickau* have their districts labeled as *Nord* (north), *Süd* (south) etc. Since we also have the 'county_name' column to distinguish them, this is currently not a dealbreaker, but we would need to consult the county_name column every time to distinguish forestry district names. This is tedious, we'd rather just use the district names by themself. Thus we add the first county name letter to the name ('M Nord', 'Z Nord' and so on) for those forestry districts only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>fdist_name</th>\n",
       "      <th>fdist_id</th>\n",
       "      <th>area_nse</th>\n",
       "      <th>area_nsne</th>\n",
       "      <th>area_se</th>\n",
       "      <th>area_sne</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M Süd</td>\n",
       "      <td>2703</td>\n",
       "      <td>392.75</td>\n",
       "      <td>4365.001441</td>\n",
       "      <td>381.91</td>\n",
       "      <td>1973.920712</td>\n",
       "      <td>POLYGON ((377329.166 5657157.286, 377285.838 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zwickau</td>\n",
       "      <td>Z Nord</td>\n",
       "      <td>2401</td>\n",
       "      <td>1319.45</td>\n",
       "      <td>4145.980567</td>\n",
       "      <td>1348.61</td>\n",
       "      <td>1555.580286</td>\n",
       "      <td>POLYGON ((305774.813 5632213.129, 305790.058 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M West</td>\n",
       "      <td>2704</td>\n",
       "      <td>36.08</td>\n",
       "      <td>1499.801018</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3411.198775</td>\n",
       "      <td>MULTIPOLYGON (((378097.915 5695126.311, 378079...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Zwickau</td>\n",
       "      <td>Z Süd</td>\n",
       "      <td>2402</td>\n",
       "      <td>1794.78</td>\n",
       "      <td>6175.259947</td>\n",
       "      <td>196.48</td>\n",
       "      <td>236.623276</td>\n",
       "      <td>POLYGON ((324855.422 5602320.960, 324853.742 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M Ost</td>\n",
       "      <td>2702</td>\n",
       "      <td>114.56</td>\n",
       "      <td>4945.945638</td>\n",
       "      <td>0.60</td>\n",
       "      <td>762.365790</td>\n",
       "      <td>POLYGON ((413698.678 5674573.351, 413686.981 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M Nord</td>\n",
       "      <td>2701</td>\n",
       "      <td>33.41</td>\n",
       "      <td>3794.019452</td>\n",
       "      <td>3.85</td>\n",
       "      <td>243.218414</td>\n",
       "      <td>POLYGON ((408736.142 5692125.831, 408767.450 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M Nord</td>\n",
       "      <td>2793</td>\n",
       "      <td>143.31</td>\n",
       "      <td>5780.407594</td>\n",
       "      <td>1.09</td>\n",
       "      <td>768.093453</td>\n",
       "      <td>POLYGON ((418952.942 5692288.782, 418909.147 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M West</td>\n",
       "      <td>2791</td>\n",
       "      <td>22.80</td>\n",
       "      <td>4255.041515</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3650.063576</td>\n",
       "      <td>POLYGON ((389635.997 5699901.234, 389648.747 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>M Süd</td>\n",
       "      <td>2792</td>\n",
       "      <td>411.13</td>\n",
       "      <td>4543.837549</td>\n",
       "      <td>381.83</td>\n",
       "      <td>1975.417673</td>\n",
       "      <td>POLYGON ((378695.051 5678837.912, 378676.082 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   county_name fdist_name  fdist_id  area_nse    area_nsne  area_se  \\\n",
       "4       Meißen      M Süd      2703    392.75  4365.001441   381.91   \n",
       "9      Zwickau     Z Nord      2401   1319.45  4145.980567  1348.61   \n",
       "14      Meißen     M West      2704     36.08  1499.801018     0.08   \n",
       "27     Zwickau      Z Süd      2402   1794.78  6175.259947   196.48   \n",
       "31      Meißen      M Ost      2702    114.56  4945.945638     0.60   \n",
       "36      Meißen     M Nord      2701     33.41  3794.019452     3.85   \n",
       "53      Meißen     M Nord      2793    143.31  5780.407594     1.09   \n",
       "54      Meißen     M West      2791     22.80  4255.041515     3.93   \n",
       "55      Meißen      M Süd      2792    411.13  4543.837549   381.83   \n",
       "\n",
       "       area_sne                                           geometry  \n",
       "4   1973.920712  POLYGON ((377329.166 5657157.286, 377285.838 5...  \n",
       "9   1555.580286  POLYGON ((305774.813 5632213.129, 305790.058 5...  \n",
       "14  3411.198775  MULTIPOLYGON (((378097.915 5695126.311, 378079...  \n",
       "27   236.623276  POLYGON ((324855.422 5602320.960, 324853.742 5...  \n",
       "31   762.365790  POLYGON ((413698.678 5674573.351, 413686.981 5...  \n",
       "36   243.218414  POLYGON ((408736.142 5692125.831, 408767.450 5...  \n",
       "53   768.093453  POLYGON ((418952.942 5692288.782, 418909.147 5...  \n",
       "54  3650.063576  POLYGON ((389635.997 5699901.234, 389648.747 5...  \n",
       "55  1975.417673  POLYGON ((378695.051 5678837.912, 378676.082 5...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# locate the fdist_name column for all foretry districts in Zwickau\n",
    "# and add the letter Z to the start of the string\n",
    "districts.loc[\n",
    "    districts['county_name'] == 'Zwickau', \n",
    "    'fdist_name'\n",
    "] = districts.loc[\n",
    "    districts['county_name'] == 'Zwickau', \n",
    "    'fdist_name'\n",
    "].map(\n",
    "    lambda x: 'Z '+ x\n",
    ")\n",
    "\n",
    "# locate the fdist_name column for all foretry districts in Meißen\n",
    "# and add the letter M to the start of the string\n",
    "districts.loc[\n",
    "    districts['county_name'] == 'Meißen', \n",
    "    'fdist_name'\n",
    "] = districts.loc[\n",
    "    districts['county_name'] == 'Meißen', \n",
    "    'fdist_name'\n",
    "].map(\n",
    "    lambda x: 'M '+ x\n",
    ")\n",
    "\n",
    "# check the results by filtering for Meißen, Zwickau\n",
    "districts[districts['county_name'].isin(['Meißen', 'Zwickau'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old and new district borders are now correctly labeled in a single geodataframe and match the fdist_id in the infestation_history dataframe. \n",
    "\n",
    "## 3.3 Supplement forestry district features\n",
    "\n",
    "From the columns in our districts geodataframe we can derive additional features. First we calculate the actual area of all districts as well as the endangered forest density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area for Kreisfreie Stadt Leipzig is 297.8 km^2, should be 297.8 km^2\n"
     ]
    }
   ],
   "source": [
    "# calculate area of the forestry district polygons in square kilometeres\n",
    "# to get the correct area, we need to use an equal area projection \n",
    "# (in this case cea) \n",
    "districts['area_fdist'] = districts.to_crs(\n",
    "    {'proj':'cea'}\n",
    ")['geometry'].area / 1000000\n",
    "\n",
    "# as a brief evaluation we check the area for the town of Leipzig \n",
    "# area should be 297.8 km^2 according to wikipedia\n",
    "kfs_leipzig_area = districts[\n",
    "    districts['county_name'] == 'Kreisfreie Stadt Leipzig'\n",
    "]['area_fdist'].sum()\n",
    "\n",
    "print(f'Area for Kreisfreie Stadt Leipzig is {kfs_leipzig_area:.1f} km^2,',\n",
    "      'should be 297.8 km^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the endangered forest density is the area \n",
    "# of non state and state endangered forest (in km^2)\n",
    "# divided by the total forestry district area\n",
    "districts['endangered_forest_density'] = (\n",
    "    districts['area_nse'] + districts['area_se']\n",
    ") * 100 / districts['area_fdist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to use the geographical information contained within the geodataframe in our model, we calculate the centroid x and y coordinates of the polygons. Thus, we get two numerical features with location information of the districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for coordinates of centroid for every district\n",
    "# could be used as features instead of dummy for every district\n",
    "districts['centroid_xcoord'] = districts[\n",
    "    'geometry'\n",
    "].map(\n",
    "    lambda x: x.centroid.coords[0][0]\n",
    ")\n",
    "\n",
    "districts['centroid_ycoord'] = districts[\n",
    "    'geometry'\n",
    "].map(\n",
    "    lambda x: x.centroid.coords[0][1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Insert missing observations in infestation history\n",
    "\n",
    "Before we merge the infestation history with the information on forestry districts, we need to pad the observations in the infestation_history dataframe. The dataset currently does not include an observation for every combination of forestry district, forest ownership and observation periods. The cause of these missing observations is that sometimes neither damaged nor disposed wood is reported. This may be the case if a forestry district does have almost no endangered forest (of the respective forest ownership type), a winter month yielded unfavorable conditions for bark beetles or the infestation subsided locally in a particular year (or a combination of these factors). According to *Sachsenforst* the appropriate procedure is to assume that for every observation not in the dataframe there was neither infested not disposed wood, because none was reported. Since this is still crucial information, we augment the dataset so that these cases are taken into account.\n",
    "\n",
    "Let's see how many rows our dataset currently has in total and how many of those rows already have neither infested nor disposed wood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially 839 observations with neither infested wood nor disposed wood (out of 8007 toal observations).\n"
     ]
    }
   ],
   "source": [
    "# how many 'zero rows' do we already have?\n",
    "n_zrows = infestation_history[\n",
    "    (infestation_history['infested_wood'] == 0) & \n",
    "    (infestation_history['disposed_wood'] == 0)\n",
    "].shape[0]\n",
    "\n",
    "print(f'Initially {n_zrows} observations with neither infested wood nor',\n",
    "      f'disposed wood (out of {infestation_history.shape[0]}',\n",
    "      f'toal observations).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in total we have 8007 observations. If we had every combination of timeframe, district and forest ownership for the years 2006 until February 2020 in the dataset, we would have 12,637 observations. We will also add the year 2005 in the dataframe, since we have the climate data for this year availabe. In case we ever want to use a moving average as a feature, we can already start with valid values for 2006. This makes the total rows we should have at the end of this chapter 13485. Calculations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after padding should be 12636 (13484 including 2005).\n"
     ]
    }
   ],
   "source": [
    "# estimate how many rows we should get after filling in zero rows\n",
    "\n",
    "# 2006 - September 2020 (without 2005)\n",
    "# 12 full years with 8 timeframes and 53 forestry districts\n",
    "# (these years are 2006-2012 and 2015-2019 = 12 total)\n",
    "# 12 remaining timeframes with 53 districts (Jan-May 2013 & Jan-Sep 2020)\n",
    "# 11 timeframes with 54 forestry districts (Jul 2013 - Sep 2014)\n",
    "# everything x2 for state & non-state forest\n",
    "\n",
    "n_full = (\n",
    "    (12 * 8 * 53) + \n",
    "    (1 * 12 * 53) + \n",
    "    (1 * 11 * 54)\n",
    ") * 2\n",
    "\n",
    "# 2005 - September 2020 (including 2005 for climate data only)\n",
    "# same as above, only one more full year\n",
    "n_full_2005 = (\n",
    "    (13 * 8 * 53) + \n",
    "    (1 * 12 * 53) + \n",
    "    (1 * 11 * 54) \n",
    ") * 2\n",
    "\n",
    "print(f'Total rows after padding should be {n_full}',\n",
    "      f'({n_full_2005} including 2005).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in the above calculation, accounting for the exact forestry district borders of every year and timeframe is somewhat complicated, but still neccessary to yield reliable results. Before we fill the dataset with missing rows, we construct one dataframe for each combination of forestry districts that existed from 2006 untill 2020. They are created from subsets of our districts_old and districts_new geodataframes. In July 2013 all districts of the *Meißen* county changed from their xx9x version to the xx0x version. In September 2014 the forestry districts of the *Erzgebirgskreis* county followed suit. So in total we have three different geographical borders, one before July 2013 (all old borders for *Meißen* and *Erzgebirgskreis*), one after September 2014 (all new borders) and one in between (*Meißen* had new borders already, *Erzgebirgskreis* still had the old ones).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for the different forestry district border changes\n",
    "# by making three geodataframes containing the right polygons\n",
    "\n",
    "# before July 2013: all old borders for Meißen and Erzgebirgskreis\n",
    "# other borders are unchanged and thus in districts_new\n",
    "districts_before_jul2013 = pd.concat(\n",
    "    [\n",
    "        districts_old, \n",
    "        districts_new[\n",
    "            (districts_new['county_name'] != 'Erzgebirgskreis') & \n",
    "            (districts_new['county_name'] != 'Meißen')\n",
    "        ]\n",
    "    ], axis=0)\n",
    "\n",
    "# bewtween July 2013 and September 2014\n",
    "# take Erzgebirgskreis from districts_old \n",
    "# and everything else from districts_new\n",
    "districts_jul2013_sep2014 = pd.concat(\n",
    "    [\n",
    "        districts_old[\n",
    "            districts_old['county_name'] == 'Erzgebirgskreis'\n",
    "        ], \n",
    "        districts_new[\n",
    "            districts_new['county_name'] != 'Erzgebirgskreis'\n",
    "        ]\n",
    "    ], axis=0)\n",
    "\n",
    "# the districts after September 2014 are already in districts_new\n",
    "districts_after_sep2014 = districts_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally fill in the missing rows for every observation with zeroes for infested and disposed wood. The first part is to define a function which creates a row based on its input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_row(df, fdist_id, year, timeframe, forest_ownership):\n",
    "    '''\n",
    "    This function takes in a dataframe, as well as values for the columns\n",
    "    'fdist_id', 'year', 'timeframe' and 'forest_ownership' of this \n",
    "    dataframe. \n",
    "    It returns a dictionary with a new row where the last two column \n",
    "    values are zero, only if there is not yet a row with the \n",
    "    other column values as specified by the inputs.\n",
    "    \n",
    "    inputs:\n",
    "        - df: the dataframe in question\n",
    "        - district: value for the 'fdist_id' column\n",
    "        - year: value for the 'year' column\n",
    "        - timeframe: value for the 'timeframe' column\n",
    "        - forest_ownership: value for the forest_ownership column\n",
    "        \n",
    "    returns:\n",
    "        - a dictionary serving as the new row in the dataframe, if the\n",
    "          row in question does not exist yet\n",
    "    '''\n",
    "    \n",
    "    # first check if there already is an observation for \n",
    "    # this combination of parameters    \n",
    "    if not (\n",
    "        (df['fdist_id'] == fdist_id) & \n",
    "        (df['year'] == year) &\n",
    "        (df['timeframe'] == timeframe) &\n",
    "        (df['forest_ownership'] == forest_ownership)\n",
    "    ).any():\n",
    "        \n",
    "        # if there is no observation yet: create one with the last two \n",
    "        # columns as zero\n",
    "        # note: this will only work if the shape and column names \n",
    "        # are exactly as they are in infestation_history\n",
    "        # however we do it this way because the function can also be used\n",
    "        # for demolition wood this way (more on that later)\n",
    "        return {\n",
    "            'fdist_id': fdist_id, \n",
    "            'year': year,\n",
    "            'timeframe': timeframe,\n",
    "            'forest_ownership': forest_ownership,\n",
    "            df.columns[-2]: 0,\n",
    "            df.columns[-1]: 0\n",
    "        } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part is to go over all possible combination of features and call the create_zero_row() function. This is done in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_fill(df=infestation_history, \n",
    "              districts_before_jul2013=districts_before_jul2013, \n",
    "              districts_jul2013_sep2014=districts_jul2013_sep2014, \n",
    "              districts_after_sep2014=districts_after_sep2014):\n",
    "    '''\n",
    "    This function takes in a dataframe and iterates over all possible\n",
    "    combinations of years, timeframes, forestry districts \n",
    "    and forest ownerships. \n",
    "    It then calls the create_zero_row function with the appicable inputs.\n",
    "    \n",
    "    inputs:\n",
    "        - df: the dataframe in question\n",
    "        - districts_before_jul2013: a geodataframe with information\n",
    "          on the forestry districts that existed before July 2013\n",
    "        - districts_jul2013_sep2014: a geodataframe with information\n",
    "          on the forestry districts that existed between July 2013\n",
    "          and September 2014\n",
    "        - districts_after_sep2014: a geodataframe with information\n",
    "          on the forestry districts that existed after September 2014\n",
    "    returns:\n",
    "        - the augemnted dataframe, filled with new rows whenever a valid \n",
    "          combination of features did not exist in df. infested_wood and \n",
    "          disposed_wood values are zero in those rows\n",
    "    '''\n",
    "    \n",
    "    # print current number of rows in df\n",
    "    print(f'Number of rows before zero_fill(): {df.shape[0]}')\n",
    "    \n",
    "    # to check every valid combination of timeframes, forest types, years \n",
    "    # and districts we use nested for loops\n",
    "    # loop through all unique months and quarters\n",
    "    for tf in df['timeframe'].unique():\n",
    "        \n",
    "        # loop through forest ownership types\n",
    "        for fo in df['forest_ownership'].unique():\n",
    "            \n",
    "            # loop through all years we want to include\n",
    "            for yr in range(2005, 2021):\n",
    "                \n",
    "                # depending on the year there were differences\n",
    "                # in forestry districts\n",
    "                # we check which year it is via an if-statement\n",
    "                \n",
    "                # before July 2013\n",
    "                if yr < 2013 or (\n",
    "                    yr == 2013 and \n",
    "                    tf in ['01 Januar-März', '04 April', \n",
    "                           '05 Mai', '06 Juni']\n",
    "                ):\n",
    "                    \n",
    "                    # loop only through the old districts before July 2013\n",
    "                    for fd in districts_before_jul2013[\n",
    "                        'fdist_id'\n",
    "                    ].unique():\n",
    "                    \n",
    "                        # create new row if conditions are met \n",
    "                        # by calling create_zero_rows()\n",
    "                        df = df.append(\n",
    "                            create_zero_row(df, fd, yr, tf, fo), \n",
    "                            ignore_index=True\n",
    "                        )\n",
    "                        \n",
    "                # between July 2013 and September 2014    \n",
    "                elif yr == 2013 or (\n",
    "                    yr == 2014 and not \n",
    "                    tf == '10 Oktober-Dezember'\n",
    "                ):\n",
    "                    \n",
    "                    # loop only through the districts from July 2013 \n",
    "                    # until December 2014\n",
    "                    for fd in districts_jul2013_sep2014[\n",
    "                        'fdist_id'\n",
    "                    ].unique():\n",
    "                    \n",
    "                        # create new row if conditions are met \n",
    "                        # by calling create_zero_rows()\n",
    "                        df = df.append(\n",
    "                            create_zero_row(df, fd,yr, tf, fo),\n",
    "                            ignore_index=True\n",
    "                        )\n",
    "                        \n",
    "                # after September 2014        \n",
    "                elif yr >= 2014 and not (\n",
    "                    yr == 2020 and \n",
    "                    tf == '10 Oktober-Dezember'\n",
    "                ):\n",
    "                    \n",
    "                    # loop only through the new districts after 2014\n",
    "                    for fd in districts_after_sep2014[\n",
    "                        'fdist_id'\n",
    "                    ].unique():\n",
    "                        \n",
    "                        # create new row if conditions are met \n",
    "                        # by calling create_zero_rows()\n",
    "                        df = df.append(\n",
    "                            create_zero_row(df, fd, yr, tf, fo), \n",
    "                            ignore_index=True\n",
    "                        )\n",
    "                        \n",
    "    # reset the index\n",
    "    df.reset_index(inplace=True, drop=True)  \n",
    "    \n",
    "    # print new number of rows\n",
    "    print(f'Number of rows after zero_fill(): {df.shape[0]}')\n",
    "          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before zero_fill(): 8007\n",
      "Number of rows after zero_fill(): 13484\n"
     ]
    }
   ],
   "source": [
    " # call zero_fill() function to augment infestation_history\n",
    "infestation_history = zero_fill(infestation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Merge the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows after inserting missing obervations matches what we calculated earlier. Now that we finally have all observations we'll ever need, we can focus on creating new features and integrating our other data sources in the infestation_history dataframe. First, we merge the info from the districts geodataframe with infestation_history. To merge, we use the fdist_id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge information on the forestry districts \n",
    "# with the obersvations in infestation_history\n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    districts[[\n",
    "        'county_name', \n",
    "        'fdist_name',\n",
    "        'area_nse', \n",
    "        'area_nsne', \n",
    "        'area_se', \n",
    "        'area_sne', \n",
    "        'fdist_id', \n",
    "        'centroid_xcoord', \n",
    "        'centroid_ycoord', \n",
    "        'area_fdist', \n",
    "        'endangered_forest_density'\n",
    "    ]], \n",
    "    on='fdist_id')\n",
    "\n",
    "# we also save the districts geodataframe as a shape file \n",
    "# this way we can access it during the EDA\n",
    "# \n",
    "# appending the geometry feature to our main dataset\n",
    "# would unnecessarily impair performance,memory and storage \n",
    "# as the information contained in the polygons are rather large\n",
    "districts.to_file('forestry_districts.shp', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Meteorological raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every meteorological parameter and every month (alternatively day), we have one raster file with a 1000 m x 1000 m raster that covers all of Saxony. Before the aggregation of the climate data takes place, we define two functions to help us in our endevour. The first one reads in a specific raster file and calculates the mean for all polygon geometries that we pass it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_mean(filename, polygons, file_errors):\n",
    "    '''\n",
    "    This function calculates the mean of values in a raster grid for \n",
    "    specific polygon shapes. \n",
    "    This is done by masking the raster grid with the polygon vectors and \n",
    "    using the masked raster points to calculate the mean.\n",
    "\n",
    "    inputs:\n",
    "        - filename: path/filename of the raster file\n",
    "        - polygons: a list of polygons that we want to calculate mean values for\n",
    "        - file_errors: the list that tracks files not found\n",
    "        \n",
    "    returns:\n",
    "        - a list with one element for every input polygon, every element being the\n",
    "          mean value of the raster masked with the polygon\n",
    "    '''  \n",
    "    # try to read in raster file\n",
    "    try:\n",
    "        current_raster = rasterio.open(filename, nodata=-9999.0)\n",
    "        \n",
    "    # specify procedure when file does not exist\n",
    "    except rasterio.errors.RasterioIOError:\n",
    "        \n",
    "        # save missing filename\n",
    "        # lists are mutable, so just append it without a return statement\n",
    "        file_errors.append(filename)\n",
    "        \n",
    "        # return NaN for all polygons\n",
    "        return [np.nan for i in range(polygons.shape[0])] \n",
    "    \n",
    "    # prepare list with results\n",
    "    results = []\n",
    "    \n",
    "    # do masking and calculation for every polygon\n",
    "    for polygon in polygons['geometry']:\n",
    "        \n",
    "        # mask raster with polygon and read in the relevant raster points\n",
    "        masked, mask_transform = mask(\n",
    "            dataset=current_raster, \n",
    "            shapes=[polygon], \n",
    "            crop=True,       # avoids loading in the whole raster\n",
    "            filled=False,    # mask outside values with nodata instead of 0, \n",
    "                             # so we can safely compute zonal stats\n",
    "            all_touched=True # overfill polygon instead of underfilling\n",
    "        ) \n",
    "        \n",
    "        # calculate the mean of the remaining raster points\n",
    "        # use numpy.ma as it supports masked arrays\n",
    "        # and append to results\n",
    "        results.append(\n",
    "            np.ma.mean(masked)\n",
    "        )\n",
    "        \n",
    "    # return results list\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function fixes a problem that stems from the fact we have different timeframe lengths. The observations are gathered monthly from April until September and quarterly for the other six months. Some climate parameterts are formulated as the sum of daily values. For example the sunshine duration 'SD0' of a month is the sum of sunshine durations of every day. Our target variable is the infested wood that was **accrued** in the timeframe, meaning it is the sum of the accrued wood for every month (technically also each day or other arbitrary period). Thus, for these quarterly timeframes we want the features in question to also be added together (for example total sunshine duration of those three months, so we can explore the relationship between sunshine duration and target variable).\n",
    "\n",
    "In case one of the files does not exist in the data (for now every file does exist, might not be the caser in production), the coorect approach would be to caluclate the numpy.nansum. However in case all of the three files for the qurterly values are missing, we want to return NaN, which is not the behavior of numpy.nansum. This why we create a wrapper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nansumwrapper(a, **kwargs):\n",
    "    '''\n",
    "    A function that returns NaN when all elements of an array-like\n",
    "    are NaN and else calculates the nansum of this array-like.\n",
    "    \n",
    "    inputs:\n",
    "        - a: any array-like\n",
    "    returns:\n",
    "        - np.nan if all elements are np.nan, else the nansum of input\n",
    "    '''\n",
    "    if np.isnan(a).all():\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.nansum(a, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the aggregation of the climate data. Since reading in thousands of raster files, overlaying vector data and doing calculations with the results is computationally expensive, we do the aggregation in way that we have to read in every file only once, do the necessary calculations and only in the end merge the results onto our main dataset. We also do not want to repeatedly append datatframes (or actually handle dataframes at all) like we did during the insertion of missing rows, where it was really an issue. \n",
    "\n",
    "Because of those reasons we do not iterate over the infestation_history dataframe nor do we use dataframes to append our interim results. As long as we calcualte the meteorological feature values we are working with a list of lists, each inner list representing one of the parameters. Only in the end, after we are done with the aggregation, we transform our results into a pandas dataframe.\n",
    "\n",
    "The following parameters exist in the data, calculated for every month. All parameters will be included as a feature with the same column name in the dataframe:\n",
    "* **TX0** - mean of the daily maximum temperatures in °C\n",
    "* **TM0** - mean temperature in °C\n",
    "* **TN0** - mean of the daily minimum temperatures in °C\n",
    "* **RF0** - mean relative humidity in %\n",
    "* **SD0** - total sunshinde duration in h\n",
    "* **RRU** - total precipitation in mm\n",
    "* **RRK** - corrected total precipitation in mm (corrects systematic errors of the measuring device and installation location such as wetting/evaporation losses)\n",
    "* **FF1** - mean of the daily mean wind velocity 10 metres above ground in m*s-1\n",
    "* **FF2** - mean of the daily mean wind velocity 2 metres above ground in m*s-1\n",
    "* **FFB** - mean of the daily wind speed of the day on the beaufort scale in bft\n",
    "* **RGK** - total global irridiation in kWh*m-2\n",
    "* **ETP** - potential evaporation in mm\n",
    "* **GRV** - potential evapotranspiration in mm\n",
    "* **KWU** - waterbalance in mm\n",
    "* **KWK** - corrected waterbalance in mm (corrects systematic errors of the measuring device and installation location such as wetting/evaporation losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2005, elapsed time: 0.00 min\n",
      "Starting with 2006, elapsed time: 1.81 min\n",
      "Starting with 2007, elapsed time: 3.63 min\n",
      "Starting with 2008, elapsed time: 5.44 min\n",
      "Starting with 2009, elapsed time: 7.23 min\n",
      "Starting with 2010, elapsed time: 8.78 min\n",
      "Starting with 2011, elapsed time: 10.44 min\n",
      "Starting with 2012, elapsed time: 12.03 min\n",
      "Starting with 2013, elapsed time: 13.60 min\n",
      "Starting with 2014, elapsed time: 15.18 min\n",
      "Starting with 2015, elapsed time: 17.04 min\n",
      "Starting with 2016, elapsed time: 18.75 min\n",
      "Starting with 2017, elapsed time: 20.32 min\n",
      "Starting with 2018, elapsed time: 21.85 min\n",
      "Starting with 2019, elapsed time: 23.37 min\n",
      "Starting with 2020, elapsed time: 24.90 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yannic\\.conda\\envs\\python377\\lib\\site-packages\\ipykernel_launcher.py:95: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished aggregation, total time: 25.16 min\n"
     ]
    }
   ],
   "source": [
    "# aggregate meteorological data\n",
    "\n",
    "# specify location of raster files\n",
    "raster_dir = r'data_raw/climate_monthly_1000/'\n",
    "\n",
    "# the obervations from april until september are gathered monthly \n",
    "# while they are gathered quarterly from october till march\n",
    "# create a dictionary that maps the timeframe values from infestation_history \n",
    "# to the naming pattern that is used in the raster file names \n",
    "timeframe_dict = {\n",
    "'01 Januar-März': ['01', '02', '03'],\n",
    "'04 April': ['04'],\n",
    "'05 Mai': ['05'],\n",
    "'06 Juni': ['06'],\n",
    "'07 Juli': ['07'],\n",
    "'08 August': ['08'],\n",
    "'09 September': ['09'],\n",
    "'10 Oktober-Dezember': ['10', '11', '12']\n",
    "}\n",
    "\n",
    "# create a dictionary of all meteorological parameter shorthands to calculate\n",
    "# these shorthands match the notation used in the respective filenames\n",
    "#\n",
    "# they are mapped to the respective aggregation function that will be used \n",
    "# if there are multiple months in the timeframe\n",
    "parameter_info = {\n",
    "    'TX0' : np.nanmean,  \n",
    "    'TM0' : np.nanmean,  \n",
    "    'TN0' : np.nanmean,  \n",
    "    'RF0' : np.nanmean,  \n",
    "    'SD0' : nansumwrapper,\n",
    "    'RRU' : nansumwrapper,\n",
    "    'RRK' : nansumwrapper,\n",
    "    'FF1' : np.nanmean,  \n",
    "    'FF2' : np.nanmean,  \n",
    "    'FFB' : np.nanmean,  \n",
    "    'RGK' : nansumwrapper,\n",
    "    'ETP' : nansumwrapper,\n",
    "    'GRV' : nansumwrapper,\n",
    "    'KWU' : nansumwrapper,\n",
    "    'KWK' : nansumwrapper\n",
    "}\n",
    "\n",
    "# since this might take a while we track the time\n",
    "# start time\n",
    "start = time.time()\n",
    "\n",
    "# use list of lists to store results\n",
    "# one inner list for every parameter plus one containing\n",
    "# the information we use to merge infestation_history on\n",
    "climate_res = [[] for _ in range(len(parameter_info) + 1)]\n",
    "\n",
    "# use list to track file not found errors\n",
    "file_errors = []\n",
    "\n",
    "# to read every file only once, iterate over the years (and timeframes) \n",
    "for current_year in np.sort(infestation_history['year'].unique()):\n",
    "\n",
    "    # whenever we start a new year, print elapsed time\n",
    "    elapsed = (time.time() - start) / 60\n",
    "    print(f'Starting with {current_year}, elapsed time: {elapsed:.2f} min')\n",
    "    \n",
    "    # if we are past 2014 we only need to go through the new districts\n",
    "    # else go through all forestry districts in the districts geodataframe\n",
    "    current_districts = districts if current_year <= 2014 else districts_new\n",
    "    \n",
    "    # get the forestry district id as well as the polygons\n",
    "    polygons = current_districts[['fdist_id', 'geometry']]\n",
    "    \n",
    "    # iterate over the timeframes\n",
    "    for current_timeframe in timeframe_dict:\n",
    "        for idx, current_parameter in enumerate(parameter_info):\n",
    "            \n",
    "            # get all filenames \n",
    "            # (one if timeframe is one month, else list has three filenames)\n",
    "            filenames = [\n",
    "                fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_' +\n",
    "                fr'{current_parameter}_MW_{current_year}' +\n",
    "                fr'{current_month}00_utm.asc' \n",
    "                for current_month in timeframe_dict.get(current_timeframe)\n",
    "            ]\n",
    "            \n",
    "            # call raster_mean() function we defined earlier\n",
    "            aggregation_results = [\n",
    "                raster_mean(\n",
    "                    filename, \n",
    "                    polygons, \n",
    "                    file_errors\n",
    "                ) for filename in filenames\n",
    "            ]\n",
    "            \n",
    "            # use suitable function from current_parameter\n",
    "            # to handle timeframes with multiple months\n",
    "            results_after_dispatch = [\n",
    "                parameter_info[current_parameter](x) for x in zip(\n",
    "                    *aggregation_results\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # result of iteration is one list single list\n",
    "            # (regardless of timeframe period)\n",
    "            # extend results\n",
    "            climate_res[idx].extend(results_after_dispatch)         \n",
    "        \n",
    "        # extend the sublist on which we merge the results by specifying \n",
    "        # what combination of year, timeframe and districts was calculated\n",
    "        merge_dummies = [\n",
    "            f'{current_year}-{current_timeframe}-{dist}' \n",
    "            for dist in polygons['fdist_id']\n",
    "        ]\n",
    "        climate_res[-1].extend(merge_dummies)\n",
    "\n",
    "\n",
    "\n",
    "# calculation done for all years, timeframes, districts and parameters\n",
    "# transform results into dataframe\n",
    "climate_res = pd.DataFrame(climate_res).T\n",
    "\n",
    "# name columns correctly\n",
    "climate_res.columns = [*parameter_info, 'merge_dummy']\n",
    "\n",
    "# print final time\n",
    "print(f'Finished aggregation, total time: {(time.time()-start)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any files were missing in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_raw/climate_monthly_1000/GRID_1_Messungen_Tageswerte_2020_TX0_MW_20200300_utm.asc',\n",
       " 'data_raw/climate_monthly_1000/GRID_1_Messungen_Tageswerte_2020_TM0_MW_20200300_utm.asc',\n",
       " 'data_raw/climate_monthly_1000/GRID_1_Messungen_Tageswerte_2020_TN0_MW_20200300_utm.asc',\n",
       " 'data_raw/climate_monthly_1000/GRID_1_Messungen_Tageswerte_2020_RF0_MW_20200300_utm.asc',\n",
       " 'data_raw/climate_monthly_1000/GRID_1_Messungen_Tageswerte_2020_SD0_MW_20200300_utm.asc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return file_errors lists\n",
    "file_errors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File errors for climate data start in 2020. In fact all climate data after February 2020 is missing, as it was not available at the time of writing. ReKIS has a delay of about half a year before publishing climate raster data for Saxony. Before February 2020 there were no errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The climate variables are still in their own dataframe, each combination of year, timeframe and forestry district assigned to 15 meteorological features in 15 columns. To merge the data to our infestation_history dataframe we contruct a merge-dummy column that distinguishes our observations in the same manner. Doing it this way is more time efficient than the alternative where reading all of the files multiple times and iterating over the dataframe would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a merge dummy analogous to merge_dummy column in climate_res\n",
    "infestation_history['merge_dummy'] = infestation_history['year'].map(\n",
    "    lambda x: str(x) + '-'\n",
    ") + infestation_history['timeframe'].map(\n",
    "    lambda x: x + '-'\n",
    ") + infestation_history['fdist_id'].astype(str)\n",
    "\n",
    "# merge with climate_res\n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    climate_res, \n",
    "    on='merge_dummy'\n",
    ").drop('merge_dummy', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature Engineering\n",
    "\n",
    "## 5.1 Preparation for time series depiction (id, timestamp)\n",
    "\n",
    "At this point it is worth interrupting the addition of other data sources to engineer some quality of life features. If we want to be able to plot the infestation development for a specific district and compare the timelines in any meaningful way, we need to add timestamps as well as a unique identifier that\n",
    "1. Logically connects old and new districts. District 2703 for example might be in another location than district 2793 due to naming conventions. However, for every districts we want to establish a continous timeline from 2006 until 2020. So for the 11 old districts that were restructured, we want to allocate the new districts whose geography best represents them. \n",
    "2. Can be used to further distinguish between state and non-state forest of this district. The id should explicitly state what county the observation is from, what the district name is and wether we are talking about the state forest or the non-state forest of this district.\n",
    "\n",
    "The goal is that every combination of id and timestamp exists exactly once, so it is unambigous what location (id) and time (timestamp) we are talking about when looking at a row/observation in the dataset. The assignment of old and new districts has already happened after wen construct the id feature, so we won'thave to worry about that inconsistency in the future. These id and timestamp features will also help us along the road, for example for calculating moving averages or when adding the demolition wood values, so it's sensible to do it as soon as possible. \n",
    "\n",
    "First, logically connect the old forestry districts to their respective new districts. For most of them we can do it by name, as for example *Marienberg* in the old structure is still best approximated by *Marienberg* in the new one. The notable exceptions are *Meißen Nord* and *Meißen West* (old), which are best approximated by *Meißen Ost* and *Meißen Nord* (new) respectively. Maps of Saxony with the new and the old districts are in the 'Preface' notebook of this project, so this assignment can be verified by the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Meißen, logically connect the old forestry districts \n",
    "# to the new ones that best approxiamte the location/shape\n",
    "def connect_districts(fdist_name, fdist_id):\n",
    "    '''\n",
    "    This function returns the name we use for construction of the id feature.\n",
    "    Because the names from old and new mostly match, just returns the name\n",
    "    in most casees.\n",
    "    \n",
    "    inputs:\n",
    "        - fdist_name: the name of the forestry district\n",
    "        - fdist_id: the id of the forestry district\n",
    "    \n",
    "    returns:\n",
    "        - the name of the new district that best represents this district\n",
    "          (almost always the same name except for Meißen)\n",
    "    '''\n",
    "    # check by id if input is not old Meißen Nord or old Meißen West\n",
    "    if not fdist_id in [2793, 2791]:\n",
    "        # return the forestry district name  \n",
    "        return fdist_name\n",
    "    \n",
    "    else:\n",
    "        # return the correct name in the new structure for Meißen\n",
    "        return fdist_name.replace(\n",
    "            # what was M Nord is almost exactly M Ost in the new structure\n",
    "            'Nord', 'Ost' \n",
    "        ).replace(\n",
    "            # M West is best approximated by M Nord in the new structure\n",
    "            'West', 'Nord' \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use connect_districts() function to make a newname feature which is \n",
    "# the name this district has in the new structure\n",
    "infestation_history['fdist_newname'] = infestation_history[[\n",
    "    'fdist_name',\n",
    "    'fdist_id'\n",
    "]].apply(\n",
    "    lambda x: connect_districts(x[0], x[1]), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# now that we have a consistent nameing convention, construct the id feature, \n",
    "# which is compromised of the county name, the new forestry district name\n",
    "# and the forest ownership group\n",
    "infestation_history['id'] = infestation_history['county_name'].map(\n",
    "    lambda x: x + '-'\n",
    ") + infestation_history['fdist_newname'].map(\n",
    "    lambda x: x + '-'\n",
    ") + infestation_history['forest_ownership']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the id done, we pursue the timestamp, which we construct from the timeframe and year columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually assign the end of month to respective timeframes\n",
    "end_of_timeframe = {\n",
    "    '01 Januar-März': '-03-31',\n",
    "    '04 April': '-04-30',\n",
    "    '05 Mai': '-05-31',\n",
    "    '06 Juni': '-06-30',\n",
    "    '07 Juli': '-07-31',\n",
    "    '08 August': '-08-31',\n",
    "    '09 September': '-09-30',\n",
    "    '10 Oktober-Dezember': '-12-31'\n",
    "    }\n",
    "             \n",
    "# make a timestamp with year and timeframes     \n",
    "infestation_history['timestamp'] = infestation_history['year'].astype(\n",
    "    str\n",
    ") + infestation_history['timeframe'].map(\n",
    "    lambda x: end_of_timeframe.get(x)\n",
    ")\n",
    "\n",
    "infestation_history['timestamp'] = pd.to_datetime(\n",
    "    infestation_history['timestamp']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Accounting for previous values of infested/disposed wood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to fit 'ordinary' regression models (as opposed to a time series based approach such as ARIMA), it is prudent to assign the previous value of infested wood for the respective forestry district as a feature. This way we can account for the historic development of the spruce bark beetle infestation. We also engineer a feature for the previous disposed wood values as well as the previous infested wood value of the other forest ownership of the same forestry district. Since beetles do generally not care fore arbitrary designated forestry ownerships, this feature helps us keep track of the whole infestation history in the forestry districts.\n",
    "\n",
    "Here, our id and timestamp feature already come in handy, as we can easily ascertain the correct previous values with their help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make features for previous infested and disposed wood\n",
    "\n",
    "# every feature will be stored in a list\n",
    "prev_disposed_wood = []\n",
    "prev_infested_wood = []\n",
    "prev_infested_wood_ofo = []\n",
    "\n",
    "# iterate over dataframe rows \n",
    "for i, row in infestation_history.iterrows():\n",
    "    \n",
    "    # for every row get row with same id and previous timestamp\n",
    "    #\n",
    "    # we still have to account for different timeframe lengths\n",
    "    # for months in April - September, take previous month\n",
    "    if row['timestamp'].month in range(4,10):\n",
    "        \n",
    "        # get previous month with same id\n",
    "        prev_row = infestation_history.loc[\n",
    "            (\n",
    "                infestation_history['timestamp'] == row[\n",
    "                    'timestamp'\n",
    "                ] + MonthEnd(-1)\n",
    "            ) & \n",
    "            (infestation_history['id'] == row['id'])\n",
    "        ]\n",
    "        \n",
    "        # to get other forest ownership, previous month with\n",
    "        # get same newname and other forest ownership\n",
    "        prev_row_ofo = infestation_history.loc[\n",
    "            (\n",
    "                infestation_history['timestamp'] == row['timestamp'] + \n",
    "                MonthEnd(-1)\n",
    "            ) & \n",
    "            (\n",
    "                infestation_history['fdist_newname'] == row['fdist_newname']\n",
    "            ) & \n",
    "            (\n",
    "                infestation_history['forest_ownership'] != row[\n",
    "                    'forest_ownership'\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # for the other months last timeframem was three months ago    \n",
    "    else:\n",
    "        # get previous timeframe with same id\n",
    "        prev_row = infestation_history.loc[\n",
    "            (\n",
    "                infestation_history['timestamp'] == row['timestamp'] + \n",
    "                MonthEnd(-3)\n",
    "            ) & \n",
    "            (\n",
    "                infestation_history['id'] == row['id']\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # to get other forest ownership, previous timeframe with\n",
    "        # get same newname and other forest ownership\n",
    "        prev_row_ofo = infestation_history.loc[\n",
    "            (\n",
    "                infestation_history['timestamp'] == row['timestamp'] + \n",
    "                MonthEnd(-3)\n",
    "            ) & \n",
    "            (\n",
    "                infestation_history['fdist_newname'] == row['fdist_newname']\n",
    "            ) & \n",
    "            (infestation_history['forest_ownership'] != row[\n",
    "                'forest_ownership']\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    # get relevant values\n",
    "    prev_disp = prev_row['disposed_wood'].values\n",
    "    prev_inf = prev_row['infested_wood'].values\n",
    "    prev_inf_ofo = prev_row_ofo['infested_wood'].values\n",
    "    \n",
    "    # append feature lists\n",
    "    prev_disposed_wood.append(\n",
    "        prev_disp[0] if len(prev_disp)==1 else np.nan\n",
    "    )\n",
    "    prev_infested_wood.append(\n",
    "        prev_inf[0] if len(prev_inf)==1 else np.nan\n",
    "    )\n",
    "    prev_infested_wood_ofo.append(\n",
    "        prev_inf_ofo[0] if len(prev_inf_ofo==1) else np.nan\n",
    "    )\n",
    "\n",
    "# insert features as columns in new dataframe    \n",
    "infestation_history['prev_disposed_wood'] = prev_disposed_wood\n",
    "infestation_history['prev_infested_wood'] = prev_infested_wood\n",
    "infestation_history['prev_infested_wood_ofo'] = prev_infested_wood_ofo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Moving averages of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous values for disposed and infested wood, moving averages/sums of the last observations may be important in portraying the infestation development up to this point. For the previously infested and disposed wood, the rolling sum of the last 12 months will be used as a new feature. For the different meteorological parameters, we will instead use the rolling mean for the last 6 summer months as one feature and the rolling mean for the 6 last winter months as a different feature. Winter and summer in this case is analogous to the split of observation periods (summer: April - September; winter: October - March). The reason for this exact approach will be made clear during the EDA. In short, a high temperature or high precipitation during winter will have different effects on the beetle population than in summer, because of the beetle phenology as well as the effect on predator species.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling sum of previously infested wood \n",
    "# and previously disposed wood of the last 12 months\n",
    "\n",
    "# save results in dataframe\n",
    "# specify column names but do not fill yet, index same as dataset\n",
    "rolling_df = pd.DataFrame(np.nan, \n",
    "                          index=range(infestation_history.shape[0]),\n",
    "                          columns=['prev_infested_wood_rollyr',\n",
    "                                   'prev_disposed_wood_rollyr']\n",
    "                         )\n",
    "\n",
    "# only take into account one id at a time\n",
    "for ID in infestation_history['id'].unique(): \n",
    "    \n",
    "    # extract relevant time series for the id, sorted by timestamps\n",
    "    id_subset = infestation_history.loc[\n",
    "        infestation_history['id'] == ID\n",
    "    ].sort_values('timestamp')\n",
    "    \n",
    "    # fill results dataframe with the rolling sum of last 8 values\n",
    "    # (the last 8 timeframes are the last 12 months)\n",
    "    #\n",
    "    # previous infested wood\n",
    "    rolling_df.loc[\n",
    "        rolling_df.index.isin(id_subset.index), \n",
    "        'prev_infested_wood_rollyr'\n",
    "    ] = id_subset['prev_infested_wood'].rolling(8).apply(\n",
    "        lambda x: np.nansum(x)\n",
    "    )\n",
    "    \n",
    "    # previous disposed wood\n",
    "    rolling_df.loc[\n",
    "        rolling_df.index.isin(id_subset.index), \n",
    "        'prev_disposed_wood_rollyr'\n",
    "    ] = id_subset['prev_disposed_wood'].rolling(8).apply(\n",
    "        lambda x: np.nansum(x)\n",
    "    )\n",
    "                          \n",
    "# sice we kept the original indeces, merge results on index        \n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    rolling_df, \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling mean of previous meteorological parameter values \n",
    "# of the last 6 summer months\n",
    "\n",
    "# save results in dataframe\n",
    "# specify column names but do not fill yet, index same as dataset\n",
    "rolling_df = pd.DataFrame(np.nan, \n",
    "                          index=range(infestation_history.shape[0]),\n",
    "                          columns=[\n",
    "                              *[name + '_rollsr' for name in parameter_info]\n",
    "                          ]\n",
    "                         )\n",
    "# only take into account one id at a time\n",
    "for ID in infestation_history['id'].unique(): \n",
    "    \n",
    "    # extract relevant time series for the id, sorted by timestamps\n",
    "    id_subset = infestation_history.loc[\n",
    "        infestation_history['id'] == ID\n",
    "    ].sort_values('timestamp')\n",
    "    \n",
    "    # give the winter months a weight of zero\n",
    "    weight = pd.Series(\n",
    "        [1 if element.month in range(\n",
    "            4, 10\n",
    "        ) else 0 for element in id_subset['timestamp']]\n",
    "    )\n",
    "        \n",
    "    # calculate moving average for every meteorological parameter\n",
    "    for current_parameter in parameter_info:\n",
    "        \n",
    "        # multiply parameter values by weight\n",
    "        # use weight.values to keep the original index from id_subset\n",
    "        weighted_parameter =  pd.Series(\n",
    "            id_subset[current_parameter] * weight.values, \n",
    "            name=current_parameter+'_rollsr'\n",
    "        )\n",
    "        \n",
    "        # perform rolling, save results in rolling_df\n",
    "        #\n",
    "        # since we multiplied the winter months by zero\n",
    "        # we get the summer mean by adding the last 8 values (12 months)\n",
    "        # together, then divide by 6 (the number of summer months)\n",
    "        rolling_df.loc[\n",
    "            rolling_df.index.isin(weighted_parameter.index), \n",
    "            current_parameter+'_rollsr'\n",
    "        ] = weighted_parameter.rolling(8).apply(\n",
    "            lambda x: np.nansum(x)/6\n",
    "        )\n",
    "                          \n",
    "# sice we kept the original indeces, merge results on index        \n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    rolling_df, \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling mean of previous meteorological parameter values \n",
    "# of the last 6 winter months\n",
    "\n",
    "# save results in dataframe\n",
    "# specify column names but do not fill yet, index same as dataset\n",
    "rolling_df = pd.DataFrame(np.nan, \n",
    "                          index=range(infestation_history.shape[0]),\n",
    "                          columns=[\n",
    "                              *[name + '_rollwr' for name in parameter_info]\n",
    "                          ]\n",
    "                         )\n",
    "# only take into account one id at a time\n",
    "for ID in infestation_history['id'].unique(): \n",
    "    \n",
    "    # extract relevant time series for the id, sorted by timestamps\n",
    "    id_subset = infestation_history.loc[\n",
    "        infestation_history['id'] == ID\n",
    "    ].sort_values('timestamp')\n",
    "    \n",
    "    # give the summer months a weight of zero\n",
    "    weight = pd.Series(\n",
    "        [0 if element.month in range(\n",
    "            4, 10\n",
    "        ) else 1 for element in id_subset['timestamp']\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    # calculate moving average for every meteorological parameter\n",
    "    for current_parameter in parameter_info:\n",
    "        \n",
    "        # multiply parameter values by weight\n",
    "        # use weight.values to keep the original index from id_subset\n",
    "        weighted_parameter =  pd.Series(\n",
    "            id_subset[current_parameter] * weight.values, \n",
    "            name=current_parameter+'_rollwr'\n",
    "        )\n",
    "        \n",
    "        # perform rolling, save results in rolling_df\n",
    "        #\n",
    "        # since we multiplied the summer months by zero\n",
    "        # we get the winter mean by adding the last 8 values (12 months)\n",
    "        # together, then divide by 6 (the number of winter months)\n",
    "        rolling_df.loc[\n",
    "            rolling_df.index.isin(weighted_parameter.index), \n",
    "            current_parameter+'_rollwr'\n",
    "        ] = weighted_parameter.rolling(8).apply(\n",
    "            lambda x: np.nansum(x)/6\n",
    "        )\n",
    "           \n",
    "# sice we kept the original indeces, merge results on index        \n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    rolling_df, \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Respective area of endangered forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations are split into state-owned and non-state-owned forest. There currently are four columns denoting how much area of forest is endangered/not endangered and state-owned/non-state-owned in the observation forestry district. To not clutter our features during model training, we create one feature with the area of endangered forest of observation's forest ownership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endangered area for this id (forestry district + correct ownership)\n",
    "infestation_history['area_endangered'] = infestation_history[\n",
    "    ['forest_ownership', 'area_nse', 'area_se']\n",
    "].apply(\n",
    "    lambda x: x[1] if x[0] == 'NSW' else x[2], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Demolition wood \n",
    "\n",
    "In this project the term 'demolition wood' summarizes all dead trees/wood/timber that was accumulated due to abiotic damages. These include but are not limited to windfall, snow, ice, lightning and drought damages. The reports are recorded in an excel-file similar to that of the infestation history, though they are always recorded in semi-annual intervals, at the end of April and September. Just like with the infestation history, we drop redundant columns and allocate the reporded wood for forestry district 2691 (*Stadtwald Zittau*) to forestry district 2601 (*Zittau*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data set\n",
    "demolition_history = pd.read_excel(\n",
    "    r'data_raw/ML_WB_20201112.xlsx', \n",
    "    names=[\n",
    "        'county_acronym', \n",
    "        'county_nr', \n",
    "        'fdist_nr', \n",
    "        'fdist_id',\n",
    "        'year', \n",
    "        'timeframe', \n",
    "        'forest_ownership', \n",
    "        'demolition_wood', \n",
    "        'disposed_demolition_wood'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just like with infestation_history, we drop the redundant columns\n",
    "demolition_history.drop(\n",
    "    ['county_acronym', 'county_nr', 'fdist_nr'], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and just like with infestation_history, \n",
    "# we need to add the Stadtwald Zittau to Zittau\n",
    "# in column 'fdist_id' change all occurrences of 2691 to 2601\n",
    "demolition_history['fdist_id'] = demolition_history['fdist_id'].replace(\n",
    "    2691, 2601\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join the data with our main dataset, we can use the fruits of our labours past. The create_zero_row() and zero_fill() functions are applicable for this dataframe and can be used to look for missing values to ultimately fill them with a reported and disposed wood of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before zero_fill(): 1945\n",
      "Number of rows after zero_fill(): 3405\n"
     ]
    }
   ],
   "source": [
    "# use the fruits of our labor to fill zero rows\n",
    "demolition_history = zero_fill(demolition_history)\n",
    "\n",
    "# since there could be manual entry errors get the new name first\n",
    "# so we can later merge back on that\n",
    "#\n",
    "# merge fdist_ids on fdist_newname to logically connect old and \n",
    "# new districts the same way we did previously\n",
    "newname_df = infestation_history[[\n",
    "    'fdist_id', 'fdist_newname'\n",
    "]].drop_duplicates('fdist_id').copy()\n",
    "\n",
    "demolition_history = pd.merge(\n",
    "    demolition_history, \n",
    "    newname_df, \n",
    "    on='fdist_id'\n",
    ")\n",
    "\n",
    "# aggregate the values by summing them together for the 'demolition_wood' \n",
    "# and 'disposed_demolition_wood' if every other column value is the same\n",
    "demolition_history['demolition_wood'] = demolition_history.groupby(\n",
    "    ['fdist_newname', 'year', 'timeframe', 'forest_ownership']\n",
    ")['demolition_wood'].transform('sum')\n",
    "\n",
    "demolition_history['disposed_demolition_wood'] = demolition_history.groupby(\n",
    "    ['fdist_newname', 'year', 'timeframe', 'forest_ownership']\n",
    ")['disposed_demolition_wood'].transform('sum')\n",
    "\n",
    "# Now drop the duplicated rows that were just created by the transformation\n",
    "demolition_history.drop_duplicates(\n",
    "    ['fdist_newname', 'year', 'timeframe', 'forest_ownership'], \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# reset the index\n",
    "demolition_history.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the demolition wood gets reported semi-anually, connection of all months that are not April or September to the previous value that was reported is still pending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function that gets previous demolition\n",
    "# reporting date based on timestamp\n",
    "\n",
    "def prev_demolition_date(timestamp):\n",
    "    '''\n",
    "    This function takes a timestamp as input and returns the last day \n",
    "    of September or April (whichever was last) as a timestamp.\n",
    "    \n",
    "    inputs:\n",
    "        - timestamp: Any timestamp\n",
    "        \n",
    "    returns:\n",
    "        - the timestamp of the last demolition reporting day\n",
    "    '''\n",
    "    # if obs is in September or April return the timestamp\n",
    "    if timestamp.month in [4, 9]:\n",
    "        return timestamp\n",
    "    # if obs is between April and September, return April of this year\n",
    "    elif timestamp.month in range(5, 9):\n",
    "        return date(timestamp.year, 4, 30)\n",
    "    # if observation is before April, return September of last year\n",
    "    elif timestamp.month in range(1, 4):\n",
    "        return date(timestamp.year - 1, 9, 30)\n",
    "    # else (after September), return September this year\n",
    "    else:\n",
    "        return date(timestamp.year, 9, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column in demolition_history dataframe analogous to infestation_history\n",
    "demolition_history['timestamp'] = demolition_history['year'].astype(\n",
    "    str\n",
    ") + demolition_history['timeframe'].map(\n",
    "    lambda x: end_of_timeframe.get(x)\n",
    ")\n",
    "\n",
    "# convert string to datetime\n",
    "demolition_history['timestamp'] = pd.to_datetime(\n",
    "    demolition_history['timestamp']\n",
    ")\n",
    "\n",
    "# use function to create a new column in infestation_history \n",
    "# with the date of the previos demolition report day\n",
    "infestation_history['demolition_date'] = infestation_history[\n",
    "    'timestamp'\n",
    "].map(\n",
    "    lambda x: prev_demolition_date(x)\n",
    ")\n",
    "\n",
    "# merge on those timestamp columns\n",
    "infestation_history = pd.merge(\n",
    "    infestation_history, \n",
    "    demolition_history[[\n",
    "        'fdist_newname',\n",
    "        'forest_ownership', \n",
    "        'timestamp', \n",
    "        'demolition_wood', \n",
    "        'disposed_demolition_wood']], \n",
    "    left_on=['fdist_newname', 'forest_ownership', 'demolition_date'], \n",
    "    right_on=['fdist_newname', 'forest_ownership', 'timestamp'], \n",
    "    suffixes=('', '_drop'),\n",
    "    how='left'\n",
    ").drop(['demolition_date', 'timestamp_drop'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Disposing ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two features remain to be engineered. They are composed of the disposing rate for the yearly infested wood and demolition wood; disposing rate meaning the amount of disposed (infested/demolition) wood divided by the amount of accrued (infested/demolition) wood. They are relatively simple to construct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there was no accrued wood, we define the disposing rate as 1 \n",
    "# (no wood remained undisposed)\n",
    "# otherwise the rate is (disposed infested wood/infested wood) and \n",
    "# (disposed demolition wood/ demolition wood)\n",
    "infestation_history['disposing_rate_demolition'] = infestation_history[[\n",
    "    'demolition_wood', 'disposed_demolition_wood'\n",
    "]].apply(\n",
    "    lambda x: 1 if x[0]==0 else x[1]/x[0], \n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "infestation_history['disposing_rate_infested_yr'] = infestation_history[[\n",
    "    'prev_infested_wood_rollyr', 'prev_disposed_wood_rollyr'\n",
    "]].apply(\n",
    "    lambda x: 1 if x[0]==0 else x[1]/x[0], \n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "# it is rare but possible that in a year more infested wood \n",
    "# was disposed than accrued for certain districts\n",
    "# \n",
    "# also rounding or entry errors may result in a rate greater than one\n",
    "#\n",
    "# overwrite these values with 1 to still keep the integrity of this feature\n",
    "infestation_history['disposing_rate_demolition'] = infestation_history[\n",
    "    'disposing_rate_demolition'\n",
    "].map(\n",
    "    lambda x: 1 if x > 1 else x\n",
    ")\n",
    "\n",
    "infestation_history['disposing_rate_infested_yr'] = infestation_history[\n",
    "    'disposing_rate_infested_yr'\n",
    "].map(\n",
    "    lambda x: 1 if x > 1 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Saving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a last look at the columns of our final data set as well as the number of NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13484 entries, 0 to 13483\n",
      "Data columns (total 74 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   fdist_id                    13484 non-null  int64         \n",
      " 1   year                        13484 non-null  int64         \n",
      " 2   timeframe                   13484 non-null  object        \n",
      " 3   forest_ownership            13484 non-null  object        \n",
      " 4   infested_wood               13484 non-null  float64       \n",
      " 5   disposed_wood               13484 non-null  float64       \n",
      " 6   county_name                 13484 non-null  object        \n",
      " 7   fdist_name                  13484 non-null  object        \n",
      " 8   area_nse                    13484 non-null  float64       \n",
      " 9   area_nsne                   13484 non-null  float64       \n",
      " 10  area_se                     13484 non-null  float64       \n",
      " 11  area_sne                    13484 non-null  float64       \n",
      " 12  centroid_xcoord             13484 non-null  float64       \n",
      " 13  centroid_ycoord             13484 non-null  float64       \n",
      " 14  area_fdist                  13484 non-null  float64       \n",
      " 15  endangered_forest_density   13484 non-null  float64       \n",
      " 16  TX0                         12848 non-null  object        \n",
      " 17  TM0                         12848 non-null  object        \n",
      " 18  TN0                         12848 non-null  object        \n",
      " 19  RF0                         12848 non-null  object        \n",
      " 20  SD0                         12848 non-null  object        \n",
      " 21  RRU                         12848 non-null  object        \n",
      " 22  RRK                         12848 non-null  object        \n",
      " 23  FF1                         12848 non-null  object        \n",
      " 24  FF2                         12848 non-null  object        \n",
      " 25  FFB                         12848 non-null  object        \n",
      " 26  RGK                         12848 non-null  object        \n",
      " 27  ETP                         12848 non-null  object        \n",
      " 28  GRV                         12848 non-null  object        \n",
      " 29  KWU                         12848 non-null  object        \n",
      " 30  KWK                         12848 non-null  object        \n",
      " 31  fdist_newname               13484 non-null  object        \n",
      " 32  id                          13484 non-null  object        \n",
      " 33  timestamp                   13484 non-null  datetime64[ns]\n",
      " 34  prev_disposed_wood          13376 non-null  float64       \n",
      " 35  prev_infested_wood          13376 non-null  float64       \n",
      " 36  prev_infested_wood_ofo      13376 non-null  float64       \n",
      " 37  prev_infested_wood_rollyr   12620 non-null  float64       \n",
      " 38  prev_disposed_wood_rollyr   12620 non-null  float64       \n",
      " 39  TX0_rollsr                  12092 non-null  float64       \n",
      " 40  TM0_rollsr                  12092 non-null  float64       \n",
      " 41  TN0_rollsr                  12092 non-null  float64       \n",
      " 42  RF0_rollsr                  12092 non-null  float64       \n",
      " 43  SD0_rollsr                  12092 non-null  float64       \n",
      " 44  RRU_rollsr                  12092 non-null  float64       \n",
      " 45  RRK_rollsr                  12092 non-null  float64       \n",
      " 46  FF1_rollsr                  12092 non-null  float64       \n",
      " 47  FF2_rollsr                  12092 non-null  float64       \n",
      " 48  FFB_rollsr                  12092 non-null  float64       \n",
      " 49  RGK_rollsr                  12092 non-null  float64       \n",
      " 50  ETP_rollsr                  12092 non-null  float64       \n",
      " 51  GRV_rollsr                  12092 non-null  float64       \n",
      " 52  KWU_rollsr                  12092 non-null  float64       \n",
      " 53  KWK_rollsr                  12092 non-null  float64       \n",
      " 54  TX0_rollwr                  12092 non-null  float64       \n",
      " 55  TM0_rollwr                  12092 non-null  float64       \n",
      " 56  TN0_rollwr                  12092 non-null  float64       \n",
      " 57  RF0_rollwr                  12092 non-null  float64       \n",
      " 58  SD0_rollwr                  12092 non-null  float64       \n",
      " 59  RRU_rollwr                  12092 non-null  float64       \n",
      " 60  RRK_rollwr                  12092 non-null  float64       \n",
      " 61  FF1_rollwr                  12092 non-null  float64       \n",
      " 62  FF2_rollwr                  12092 non-null  float64       \n",
      " 63  FFB_rollwr                  12092 non-null  float64       \n",
      " 64  RGK_rollwr                  12092 non-null  float64       \n",
      " 65  ETP_rollwr                  12092 non-null  float64       \n",
      " 66  GRV_rollwr                  12092 non-null  float64       \n",
      " 67  KWU_rollwr                  12092 non-null  float64       \n",
      " 68  KWK_rollwr                  12092 non-null  float64       \n",
      " 69  area_endangered             13484 non-null  float64       \n",
      " 70  demolition_wood             13374 non-null  float64       \n",
      " 71  disposed_demolition_wood    13374 non-null  float64       \n",
      " 72  disposing_rate_demolition   13374 non-null  float64       \n",
      " 73  disposing_rate_infested_yr  12620 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(50), int64(2), object(21)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# show info on dataframe\n",
    "infestation_history.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be in order. We save the dataset, ready to be used in the EDA and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset as csv\n",
    "infestation_history.to_csv('barkbeetle_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
