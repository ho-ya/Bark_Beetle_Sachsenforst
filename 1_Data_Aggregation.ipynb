{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import rasterio \n",
    "import rasterio.plot\n",
    "from rasterio.mask import mask\n",
    "from glob import glob\n",
    "import time\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infestation_history = pd.read_excel(r'data_raw/ML_BDR_20201019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LK</th>\n",
       "      <th>LK-Nr</th>\n",
       "      <th>LK-Rev</th>\n",
       "      <th>REVUFBADR</th>\n",
       "      <th>Jahr</th>\n",
       "      <th>ZR</th>\n",
       "      <th>Eigentumsgruppe</th>\n",
       "      <th>Zugang</th>\n",
       "      <th>Abgang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>06 Juni</td>\n",
       "      <td>SW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>08 August</td>\n",
       "      <td>SW</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2007</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2008</td>\n",
       "      <td>04 April</td>\n",
       "      <td>SW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BZ</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2501</td>\n",
       "      <td>2008</td>\n",
       "      <td>06 Juni</td>\n",
       "      <td>SW</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LK  LK-Nr  LK-Rev  REVUFBADR  Jahr                   ZR Eigentumsgruppe  \\\n",
       "0  BZ     25       1       2501  2007              06 Juni              SW   \n",
       "1  BZ     25       1       2501  2007            08 August              SW   \n",
       "2  BZ     25       1       2501  2007  10 Oktober-Dezember              SW   \n",
       "3  BZ     25       1       2501  2008             04 April              SW   \n",
       "4  BZ     25       1       2501  2008              06 Juni              SW   \n",
       "\n",
       "   Zugang  Abgang  \n",
       "0     5.0     0.0  \n",
       "1    12.0    12.0  \n",
       "2     2.0     0.0  \n",
       "3     1.0     0.0  \n",
       "4     2.0     0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infestation_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forestry Districts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'REVUFBADR' column contains a unique identifier for the forstry districts. The first two digits indicate the county (Landkreis) and the last two digits indicate the number of the district in this county. \n",
    "\n",
    "In some forestry districts the district number (last two digits) begins with a leading 9 instead of a leading 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 1101,\n",
       "       1201, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2191, 2192, 2193,\n",
       "       2194, 2195, 2196, 2197, 2198, 2201, 2202, 2203, 2204, 2601, 2602,\n",
       "       2603, 2604, 2605, 2606, 2691, 2901, 2902, 2701, 2702, 2703, 2704,\n",
       "       2791, 2792, 2793, 2801, 2802, 2803, 2804, 2805, 3001, 3002, 3003,\n",
       "       2301, 2302, 2303, 2304, 2305, 2306, 2401, 2402], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display all forestry district numbers\n",
    "infestation_history.REVUFBADR.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the observation timeframe, some of the districts (*Erzgebirgskreis* and *Meißen*) underwent a restructuring process. A leading 9 instead of a leading 0 signifies that the border of the district was different than it is today.  We can see when these changes happened with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LK</th>\n",
       "      <th>LK-Nr</th>\n",
       "      <th>LK-Rev</th>\n",
       "      <th>Jahr</th>\n",
       "      <th>ZR</th>\n",
       "      <th>Eigentumsgruppe</th>\n",
       "      <th>Zugang</th>\n",
       "      <th>Abgang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REVUFBADR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>91</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>4430.82</td>\n",
       "      <td>4701.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>92</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>2029.06</td>\n",
       "      <td>2185.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>93</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>231.00</td>\n",
       "      <td>238.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>94</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>445.00</td>\n",
       "      <td>460.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>95</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>1238.38</td>\n",
       "      <td>1219.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>96</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>1140.88</td>\n",
       "      <td>1157.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>97</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>1035.00</td>\n",
       "      <td>1268.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>ERZ</td>\n",
       "      <td>21</td>\n",
       "      <td>98</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>175.75</td>\n",
       "      <td>164.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>GR</td>\n",
       "      <td>26</td>\n",
       "      <td>91</td>\n",
       "      <td>2020</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>NSW</td>\n",
       "      <td>18000.00</td>\n",
       "      <td>15200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>MEI</td>\n",
       "      <td>27</td>\n",
       "      <td>91</td>\n",
       "      <td>2013</td>\n",
       "      <td>01 Januar-März</td>\n",
       "      <td>NSW</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792</th>\n",
       "      <td>MEI</td>\n",
       "      <td>27</td>\n",
       "      <td>92</td>\n",
       "      <td>2013</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>271.25</td>\n",
       "      <td>209.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>MEI</td>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>2013</td>\n",
       "      <td>10 Oktober-Dezember</td>\n",
       "      <td>SW</td>\n",
       "      <td>70.00</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LK  LK-Nr  LK-Rev  Jahr                   ZR Eigentumsgruppe  \\\n",
       "REVUFBADR                                                                  \n",
       "2191       ERZ     21      91  2014  10 Oktober-Dezember              SW   \n",
       "2192       ERZ     21      92  2014  10 Oktober-Dezember              SW   \n",
       "2193       ERZ     21      93  2014  10 Oktober-Dezember              SW   \n",
       "2194       ERZ     21      94  2014  10 Oktober-Dezember              SW   \n",
       "2195       ERZ     21      95  2014  10 Oktober-Dezember              SW   \n",
       "2196       ERZ     21      96  2014  10 Oktober-Dezember              SW   \n",
       "2197       ERZ     21      97  2014  10 Oktober-Dezember              SW   \n",
       "2198       ERZ     21      98  2014  10 Oktober-Dezember              SW   \n",
       "2691        GR     26      91  2020  10 Oktober-Dezember             NSW   \n",
       "2791       MEI     27      91  2013       01 Januar-März             NSW   \n",
       "2792       MEI     27      92  2013  10 Oktober-Dezember              SW   \n",
       "2793       MEI     27      93  2013  10 Oktober-Dezember              SW   \n",
       "\n",
       "             Zugang    Abgang  \n",
       "REVUFBADR                      \n",
       "2191        4430.82   4701.61  \n",
       "2192        2029.06   2185.31  \n",
       "2193         231.00    238.00  \n",
       "2194         445.00    460.00  \n",
       "2195        1238.38   1219.88  \n",
       "2196        1140.88   1157.92  \n",
       "2197        1035.00   1268.00  \n",
       "2198         175.75    164.35  \n",
       "2691       18000.00  15200.00  \n",
       "2791          15.00     15.00  \n",
       "2792         271.25    209.75  \n",
       "2793          70.00     72.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infestation_history[infestation_history['LK-Rev'] >= 90].groupby('REVUFBADR').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to do this grouping by 'LK-Rev' because the two parts of 'REVUFBADR' also appear in the 'LK-Nr' and 'LK-Rev' columns seperately. This also means that they are redundant. We check if the information the three columns contain are really the same for every observation and then drop 'LK-Nr' and 'LK-Rev':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first column as a string\n",
    "lk_nr = infestation_history['LK-Nr'].astype(str) \n",
    "# second column as a string with leading zero\n",
    "lk_rev = infestation_history['LK-Rev'].astype(str).apply(lambda x: x.zfill(2)) \n",
    "\n",
    "# concatenate these strings and check if they are identical to the 'REVUFBADR' column at every observation\n",
    "(lk_nr + lk_rev == infestation_history['REVUFBADR'].astype(str)).all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'LK-Nr' and 'LK-Rev' columns because the information is also found in 'REVUFBADR'\n",
    "infestation_history.drop(['LK-Nr', 'LK-Rev', 'LK'], axis=1, inplace=True)\n",
    "\n",
    "# TODO: remove the following comments or keep 'LK'\n",
    "# the 'LK' column is also redundant as it contains a string that matches its 'LK-Rev' column\n",
    "# since we can use it in the EDA more intuitively than just the different 'REVUFBADR' numbers we will keep it for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we continue examining the cases with leading 9s. The *Stadtwald Zittau* (REVUFBADR 2691) is a special case among those special cases. According to Sachsenforst the correct procedure is to just add the corresponding observations to the forestry district *Zittau* (REVUFBADR 2601)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in column 'REVUFBADR' change all occurrences of 2691 to 2601\n",
    "infestation_history['REVUFBADR'] = infestation_history['REVUFBADR'].replace(2691, 2601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the values by summing them together for the 'Zugang' and 'Abgang' columns if every other column value is the same\n",
    "infestation_history['Zugang'] = infestation_history.groupby(['REVUFBADR', 'Jahr', 'ZR', 'Eigentumsgruppe'])['Zugang'].transform('sum')\n",
    "infestation_history['Abgang'] = infestation_history.groupby(['REVUFBADR', 'Jahr', 'ZR', 'Eigentumsgruppe'])['Abgang'].transform('sum')\n",
    "\n",
    "# Now drop the duplicated rows that were just created\n",
    "infestation_history.drop_duplicates(inplace=True)\n",
    "\n",
    "# reset the index\n",
    "infestation_history.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8008, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infestation_history.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining forestry districts we need to distinguish between the old borders and the new ones. Sachsenforst supplied us with two shape files, one with all current district borders and one with only borders of old districts that were different than they are now. We only have to change the 'REVUFBADR' numbers for the abolished districts so they match the format with the leading 9s and then merge both geodataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KREIS_NAME</th>\n",
       "      <th>REVUFB_NM</th>\n",
       "      <th>REVUFBADR</th>\n",
       "      <th>NSW_FI</th>\n",
       "      <th>NSW_SONST</th>\n",
       "      <th>SW_FI</th>\n",
       "      <th>SW_SONST</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mittelsachsen</td>\n",
       "      <td>Reinsberg</td>\n",
       "      <td>2203</td>\n",
       "      <td>1597.32</td>\n",
       "      <td>3274.630917</td>\n",
       "      <td>2706.18</td>\n",
       "      <td>2133.910411</td>\n",
       "      <td>POLYGON ((386902.476 5656907.025, 386910.595 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mittelsachsen</td>\n",
       "      <td>Geringswalde</td>\n",
       "      <td>2201</td>\n",
       "      <td>841.61</td>\n",
       "      <td>3508.605810</td>\n",
       "      <td>196.15</td>\n",
       "      <td>1453.972847</td>\n",
       "      <td>POLYGON ((332902.962 5650328.573, 332905.989 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leipzig</td>\n",
       "      <td>Leipziger Land</td>\n",
       "      <td>2902</td>\n",
       "      <td>401.71</td>\n",
       "      <td>8199.853850</td>\n",
       "      <td>615.51</td>\n",
       "      <td>5314.476829</td>\n",
       "      <td>POLYGON ((332897.160 5650325.466, 332893.592 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      KREIS_NAME       REVUFB_NM REVUFBADR   NSW_FI    NSW_SONST    SW_FI  \\\n",
       "0  Mittelsachsen       Reinsberg      2203  1597.32  3274.630917  2706.18   \n",
       "1  Mittelsachsen    Geringswalde      2201   841.61  3508.605810   196.15   \n",
       "2        Leipzig  Leipziger Land      2902   401.71  8199.853850   615.51   \n",
       "\n",
       "      SW_SONST                                           geometry  \n",
       "0  2133.910411  POLYGON ((386902.476 5656907.025, 386910.595 5...  \n",
       "1  1453.972847  POLYGON ((332902.962 5650328.573, 332905.989 5...  \n",
       "2  5314.476829  POLYGON ((332897.160 5650325.466, 332893.592 5...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the first shape file as a geopandas geodataframe\n",
    "districts_new = gpd.read_file(r'data_raw/shape/ufb_rev_wald_teil.shp', encoding='utf-8')\n",
    "districts_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KREIS_NAME</th>\n",
       "      <th>REVUFB_NM</th>\n",
       "      <th>REVUFBADR</th>\n",
       "      <th>NSW_FI</th>\n",
       "      <th>NSW_SONST</th>\n",
       "      <th>SW_FI</th>\n",
       "      <th>SW_SONST</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>Nord</td>\n",
       "      <td>2703</td>\n",
       "      <td>143.31</td>\n",
       "      <td>5780.407594</td>\n",
       "      <td>1.09</td>\n",
       "      <td>768.093453</td>\n",
       "      <td>POLYGON ((418952.942 5692288.782, 418909.147 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>West</td>\n",
       "      <td>2701</td>\n",
       "      <td>22.80</td>\n",
       "      <td>4255.041515</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3650.063576</td>\n",
       "      <td>POLYGON ((389635.997 5699901.234, 389648.747 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meißen</td>\n",
       "      <td>Süd</td>\n",
       "      <td>2702</td>\n",
       "      <td>411.13</td>\n",
       "      <td>4543.837549</td>\n",
       "      <td>381.83</td>\n",
       "      <td>1975.417673</td>\n",
       "      <td>POLYGON ((378695.051 5678837.912, 378676.082 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  KREIS_NAME REVUFB_NM REVUFBADR  NSW_FI    NSW_SONST   SW_FI     SW_SONST  \\\n",
       "0     Meißen      Nord      2703  143.31  5780.407594    1.09   768.093453   \n",
       "1     Meißen      West      2701   22.80  4255.041515    3.93  3650.063576   \n",
       "2     Meißen       Süd      2702  411.13  4543.837549  381.83  1975.417673   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((418952.942 5692288.782, 418909.147 5...  \n",
       "1  POLYGON ((389635.997 5699901.234, 389648.747 5...  \n",
       "2  POLYGON ((378695.051 5678837.912, 378676.082 5...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the second shape file as a geopandas geodataframe\n",
    "districts_old = gpd.read_file(r'data_raw/shape/ufb_rev_vorUmstrukturierungen.shp', encoding='utf-8')\n",
    "\n",
    "districts_old.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 90 to every 'REVUFBADR' in the districts_old dataframe to get the leading 9 notation for abolished forestry districts\n",
    "districts_old['REVUFBADR'] = districts_old['REVUFBADR'].astype(int) + 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change 'REVUFBADR' of districts_new to type int\n",
    "districts_new['REVUFBADR'] = districts_new['REVUFBADR'].astype(int)\n",
    "\n",
    "# merge the geodataframes\n",
    "districts = pd.merge(districts_new, districts_old, how ='outer') \n",
    "\n",
    "# shape should be 64x8 now\n",
    "districts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old and new district borders are now present as well as correctly labeled in both the geodata and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for coordinates of centroid for every district\n",
    "# maybe useful as features instead of dummy for every district\n",
    "districts['centroid_xcoord'] = districts['geometry'].apply(lambda x: x.centroid.coords[0][0])\n",
    "districts['centroid_ycoord'] = districts['geometry'].apply(lambda x: x.centroid.coords[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Kommentieren\n",
    "districts_before_jul2013 = pd.concat([districts_old, districts_new[(districts_new['KREIS_NAME'] != 'Erzgebirgskreis') & (districts_new['KREIS_NAME'] != 'Meißen')]], axis=0)\n",
    "districts_jul2013_sep2014 = pd.concat([districts_old[districts_old['KREIS_NAME'] == 'Erzgebirgskreis'], districts_new[districts_new['KREIS_NAME'] != 'Erzgebirgskreis']], axis=0)\n",
    "districts_after_sep2014 = districts_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating the different datasets\n",
    "\n",
    "For this project there are three different data sources. \n",
    "\n",
    "The data sources are:\n",
    "1. **The infestation history**\n",
    "    * contains all observations for the amount of damaged wood (target variable)\n",
    "    * also contains the timeframe for these observations, the respective forestry district, the type of forest (sepeartion by private/state owned) and the amount of refurbished wood in this time period\n",
    "    * data supplied by Sachsenforst\n",
    "    * already read in and stored in the infestation_history dataframe\n",
    "\n",
    "\n",
    "2. **Information on the forestry districts (new and old)**\n",
    "    * contains the geodata (polygons) of these districts\n",
    "    * also for every district contains the area covered by forest, separated by private/state owned forest as well as endangered and safe forest area (endangered are only sections that consist predominantely of adult spruce trees)\n",
    "    * data supplied by Sachsenforst\n",
    "    * already read in and stored in the districts geodataframe\n",
    "\n",
    "\n",
    "3. **Meteorological raster data**\n",
    "    * contain certain climatic parameters such as the maximum, mean, minimum temperature, humidity, wind speeds etc. (15 variables total)\n",
    "    * one raster file for every variable and every day of the covered time period (from January 2006 up to February 2020, so more than 80,000 files)\n",
    "    * 5000mx5000m raster\n",
    "    * supplied by ReKIS (*Regionales Klima-Informationssystem Sachsen, Sachsen-Anhalt und Thüringen*, https://rekis.hydro.tu-dresden.de/)\n",
    "\n",
    "To make sense of the data we will have to aggregate this information into a single dataframe that can be used for an EDA and the modeling process. This will be done in the following sections.\n",
    "\n",
    "The data aggreagtion will take place in a function that iterates over the rows (observations) of our infestation_history dataframe and supplements them with the information from the other data sources. The infestation_history dataframe was chosen as the skeleton on which information is added on because of the iterative nature of the data science life cycle. In case we later drop observations from the get go, create new synthetic observations or engineer our features differently, we need to ensure that this function still operates as expected. Thus the approach of taking infestation_history as the base and then specifying what to do with the rest of the data for every observation was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially 839 observations with neither damaged wood nor restored wood.\n"
     ]
    }
   ],
   "source": [
    "# how many zero rows do we already have?\n",
    "n_zrows = infestation_history[(infestation_history['Zugang'] == 0) & (infestation_history['Abgang'] == 0)].shape[0]\n",
    "print(f'Initially {n_zrows} observations with neither damaged wood nor restored wood.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_row(obs, district, year, timeframe, forest_type):\n",
    "    '''\n",
    "    TODO: Add description\n",
    "    '''\n",
    "    # first check if there already is an observation for this combination of parameters    \n",
    "    if not (\n",
    "        (obs['REVUFBADR'] == district) & \n",
    "        (obs['Jahr'] == year) &\n",
    "        (obs['ZR'] == timeframe) &\n",
    "        (obs['Eigentumsgruppe'] == forest_type)\n",
    "    ).any():\n",
    "        \n",
    "        # if there is no observation yet: create one with damaged wood (Zugang) and restored wood (Abgang) of 0\n",
    "        return {\n",
    "            'REVUFBADR': district, \n",
    "            'Jahr': year,\n",
    "            'ZR': timeframe,\n",
    "            'Eigentumsgruppe': forest_type,\n",
    "            'Zugang': 0,\n",
    "            'Abgang': 0\n",
    "        } \n",
    "\n",
    "\n",
    "def zero_fill(obs=infestation_history, districts_before_jul2013=districts_before_jul2013, districts_jul2013_sep2014=districts_jul2013_sep2014, districts_after_sep2014=districts_after_sep2014):\n",
    "    '''\n",
    "    TODO: Add description\n",
    "    '''\n",
    "    \n",
    "    # print current number of rows\n",
    "    print(f'Number of rows before zero_fill(): {obs.shape[0]}')\n",
    "    \n",
    "    # to check every valid combination of timeframes, forest types, years and districts we use nested loops\n",
    "    # loop through all unique months and quarters\n",
    "    for timeframe in obs['ZR'].unique():\n",
    "        \n",
    "        # loop through both types of forest (state owned - SW, private - NSW)\n",
    "        for forest_type in obs['Eigentumsgruppe'].unique():\n",
    "            \n",
    "            # loop through all years\n",
    "            for year in obs['Jahr'].unique():\n",
    "                \n",
    "                # depending on the year there were different forestry districts\n",
    "                # we check which year it is via an if-statement\n",
    "                if year < 2013 or (year == 2013 and timeframe in ['01 Januar-März', '04 April', '05 Mai', '06 Juni']):\n",
    "                    \n",
    "                    # loop only through the old districts before July 2013\n",
    "                    for district in districts_before_jul2013['REVUFBADR'].unique():\n",
    "                    \n",
    "                        # create new row if conditions are met by calling create_zero_rows()\n",
    "                        obs = obs.append(\n",
    "                            create_zero_row(obs, district, year, timeframe, forest_type),\n",
    "                            ignore_index=True)\n",
    "                    \n",
    "                elif year == 2013 or (year == 2014 and not timeframe == '10 Oktober-Dezember'):\n",
    "                    \n",
    "                    # loop only through the districts from July 2013 until December 2014\n",
    "                    for district in districts_jul2013_sep2014['REVUFBADR'].unique():\n",
    "                    \n",
    "                        # create new row if conditions are met by calling create_zero_rows()\n",
    "                        obs = obs.append(\n",
    "                            create_zero_row(obs, district, year, timeframe, forest_type),\n",
    "                            ignore_index=True)\n",
    "                        \n",
    "                elif year >= 2014:\n",
    "\n",
    "                    # additionial check to ensure we do not add rows after september 2020 (end of observations)\n",
    "                    if not (year == 2020 and timeframe == '10 Oktober-Dezember'):\n",
    "                        \n",
    "                        # loop only through the new districts after 2014\n",
    "                        for district in districts_after_sep2014['REVUFBADR'].unique():\n",
    "                            \n",
    "                            # create new row if conditions are met by calling create_zero_rows()\n",
    "                            obs = obs.append(\n",
    "                                create_zero_row(obs, district, year, timeframe, forest_type),\n",
    "                                ignore_index=True)\n",
    "       \n",
    "    # reset the index\n",
    "    obs.reset_index(inplace=True, drop=True)  \n",
    "    \n",
    "    # print new number of rows\n",
    "    print(f'Number of rows after zero_fill(): {obs.shape[0]}')\n",
    "          \n",
    "    return obs\n",
    "\n",
    "# TODO: replace if statements with mapping dictionairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before zero_fill(): 8008\n",
      "Number of rows after zero_fill(): 12637\n"
     ]
    }
   ],
   "source": [
    "infestation_history = zero_fill(infestation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12637"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 53 * 8 * 12 # \n",
    "d = 53 * 5 # 2013\n",
    "b = 54 * 11 # 2013/2014\n",
    "c = 53 * 7 #2020\n",
    "\n",
    "\n",
    "2*(d+a+b+c)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_mean(filename, polygon):\n",
    "    '''\n",
    "    This function calculates the mean of a target meteorological parameter for a specific polygon over a given timeframe. \n",
    "    This is done by masking the rasters with the polygon and using the masked raster points to calculate the mean.\n",
    "\n",
    "    inputs:\n",
    "        - raster_dir: directory where all meteorological raster files are stored\n",
    "        - polygon: shape of the forestry district\n",
    "        - parameter_name: the shorthand for the meteorological parameter (needs to match the shorthand in the raster file names)\n",
    "        - year: the year of the obervation\n",
    "        - timeframe: the timeframe of the observation, formatted in a way that the glob() function can identify the right files based on a pattern match (example: '0[1-3]' for january-march)\n",
    "        \n",
    "    returns:\n",
    "        - the mean value of the meteorological parameter for the timeframe in the specified forestry district\n",
    "    '''  \n",
    "    # TODO: Workaround entfernen wenn alle Datein vorhanden\n",
    "    try:\n",
    "        current_raster = rasterio.open(filename, nodata=-9999.0)\n",
    "        \n",
    "        # mask raster with polygon and read in the relevant raster points\n",
    "        masked, mask_transform = mask(\n",
    "            dataset=current_raster, \n",
    "            shapes=[polygon], \n",
    "            crop=True, # avoids loading in the whole raster\n",
    "            filled=False, # mask outside values with nodata instead of 0, so we can safely compute zonal stats\n",
    "            all_touched=True # we can chose to overfill or underfill the polygon, in this case we overfill\n",
    "        ) \n",
    "    except: \n",
    "        masked = [np.nan]\n",
    "    \n",
    "    \n",
    "    # since we want to return the mean of the parameter over the whole timeframe we return the arithmetic mean of the list of daily values\n",
    "    return np.ma.mean(masked)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function in which the data aggregation takes place\n",
    "\n",
    "def data_aggregation(obs=infestation_history, forestry_districts=districts, raster_dir=r'data_raw/climate_33_months_1000/'):\n",
    "    '''\n",
    "    This function iterates over the rows (observations) of the obs dataframe (in our case infestation_history) and supplements them with the information from the other data sources.\n",
    "    If we do feature engineering that requires meteorological data of a higher time resolution than the observation timeframe it is also done in this dunction.\n",
    "    (for example: new feature that contains the number of days with a maximum temperature below 8 degrees Celsius)\n",
    "    \n",
    "    inputs:\n",
    "        - obs: main dataframe containing observations of (among other things) the target variable \n",
    "        - forestry_districts: dataframe containing the geodata and further information on the forestry districts\n",
    "        - raster_dir: directory where all meteorological raster files are stored\n",
    "        \n",
    "    returns:\n",
    "        - a single dataframe with the aggregated information that can be used for the EDA and modeling process\n",
    "    '''\n",
    "    obs = obs.loc[:1000]\n",
    "    start_time = time.time()\n",
    "    # create an empty dataframe in which we will store our new features\n",
    "    new_features = pd.DataFrame()\n",
    "    \n",
    "    # create a list of all meteorological parameter shorthands that we want to calculate the mean for\n",
    "    parameter_names = [\n",
    "        'TX0', # maximum temperature of the day in degrees Celsius\n",
    "        'TM0', # mean temperature of the day in degrees Celsius\n",
    "        'TN0', # minimum temperature of the day in degrees Celsius\n",
    "        'RF0', # mean relative humidity of the day in %\n",
    "        'SD0', # total sunshine duration of the day in h\n",
    "        'RRU', # total precipitation of the day in mm\n",
    "        'RRK', # corrected total precipitation of the day in mm (corrects systematic errors of the measuring device and installation location such as wetting/evaporation losses)\n",
    "        'FF1', # mean wind velocity of the day 10 metres above ground in m*s-1\n",
    "        'FF2', # mean wind velocity of the day 2 metres above ground in m*s-1\n",
    "        'FFB', # wind speed of the day on the beaufort scale in bft\n",
    "        'RGK', # total global solar irradiation of the day in kWh*m-2\n",
    "        'ETP', # potential evaporation for the day in mm\n",
    "        'GRV', # potential evapotranspiration for the day in mm\n",
    "        'KWU',\n",
    "        'KWK'\n",
    "    ]\n",
    "    \n",
    "    # the obervations from april till september are gathered monthly while they are gathered quarterly from october till march\n",
    "    # create a dictionairy that maps the timeframe values from infestation_history to the pattern that is used in the raster file names \n",
    "    timeframe_dict = {\n",
    "    '01 Januar-März': ['01', '02', '03'],\n",
    "    '04 April': ['04'],\n",
    "    '05 Mai': ['05'],\n",
    "    '06 Juni': ['06'],\n",
    "    '07 Juli': ['07'],\n",
    "    '08 August': ['08'],\n",
    "    '09 September': ['09'],\n",
    "    '10 Oktober-Dezember': ['10', '11', '12']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # initiate for loop, as we do multiple calculations per row for every row\n",
    "    for current_index, current_obs in obs.iterrows():\n",
    "        \n",
    "        # provide the current progress to user after every 500 rows\n",
    "        if current_index % 200 == 0:\n",
    "            print(f'currently at index {current_index}, elapsed time: {time.time()-start_time}')\n",
    "        \n",
    "        # create a dictionairy in which all features of the current iteration will be collected\n",
    "        feature_dict = {}\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 1-4: AREAS COVERED BY DIFFERENT TYPES OF FOREST\n",
    "        # get respective forest areas from forestry_districts\n",
    "        \n",
    "        # area of non-stateowned, non-endangered forest \n",
    "        feature_dict['area_nsne'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'NSW_SONST'].item()\n",
    "        # area of non-stateowned, endangered forest \n",
    "        feature_dict['area_nse'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'NSW_FI'].item()\n",
    "        # area of stateowned, non-endangered forest \n",
    "        feature_dict['area_sne'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'SW_SONST'].item()\n",
    "        # area of stateowned, endangered forest \n",
    "        feature_dict['area_se'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'SW_FI'].item()\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 5-6: GEOGRAPHICAL CENTROID COORDINATES\n",
    "        # these two continous variables can maybe used instead of a categorial dummy variable for every forestry district\n",
    "        # already calculated for every district in the geodataframe, we only need make it a feature in our new_features dictionairy\n",
    "        \n",
    "        # get centroid x coordinate \n",
    "        feature_dict['centroid_xcoord'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'centroid_xcoord'].item()\n",
    "        # get centroid y coordinate \n",
    "        feature_dict['centroid_ycoord'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'centroid_ycoord'].item()\n",
    "    \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 7-8: FORESTRY DISTRICT AND COUNTY NAMES\n",
    "        # supplement 'REVUFBADR' with the respective county and forestry district name\n",
    "        # these will not be used in model but do enable enable a more descriptive EDA \n",
    "        \n",
    "        # because the difference is almost indistinguishable we simplify our districts by using the 'new' district names for their pre restructuring counterparts\n",
    "        # this makes a qualitative analysis more clear because most of the actual forest in these districts is still the same\n",
    "        \n",
    "        # get leading 0 notation of current 'REVUFBADR' by replacing 9 with 0 for the third digit\n",
    "        current_district = str(current_obs['REVUFBADR'])\n",
    "        query_district = current_district[:2]+current_district[2].replace('9','0')+current_district[3]\n",
    "        #### special case 2198 (Schwarzenberg), does not exist in new structure, so we allocate it to 2101 (Eibenstock)\n",
    "        ####query_district = int(current_district.replace('2108', '2101'))\n",
    "        # special case 2198 (Schwarzenberg), does not exist in new structure, so we leave it\n",
    "        query_district = int(current_district.replace('2108', '2198'))\n",
    "        \n",
    "        # get corresponding county name\n",
    "        feature_dict['county_name'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == query_district, 'KREIS_NAME'].item()\n",
    "        # get corresponding district name\n",
    "        feature_dict['district_name'] = forestry_districts.loc[forestry_districts['REVUFBADR'] == query_district, 'REVUFB_NM'].item()\n",
    "        \n",
    "\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 7-X: MEANS OF THE DIFFERENT METEOROLOGICAL PARAMETERS DURING THE OBSERVATION TIMEFRAME\n",
    "        # even if we later do more sophisticated feature enginnering, the mean for every meteorological parameters will serve as a decent starting point for the analysis\n",
    "        \n",
    "        # the raster_mean() function is already defined, we just need to pass it the specifics of the current observation\n",
    "        # get polygon for current observation from the geodataframe\n",
    "        current_polygon = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'geometry'].item()\n",
    "        # get year for curent observation from obs\n",
    "        current_year = current_obs['Jahr']\n",
    "        # to get the timeframe for the current observation in the right format we use timeframe_dict\n",
    "        current_timeframe = timeframe_dict.get(current_obs['ZR'])\n",
    "        \n",
    "        # raster_mean() function call for every meteorological parameter we want to use\n",
    "        for current_parameter in parameter_names:\n",
    "            # construct the right filename(s) for the current observation\n",
    "            filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_{current_parameter}_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "            \n",
    "            # function call and assignment to feature_dict\n",
    "            feature_dict[f'{current_parameter}_mean'] = np.nanmean([raster_mean(filename, current_polygon) for filename in filenames])\n",
    "\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # STORE ALL FEATURES OF CURRENT OBSERVATION IN DATAFRAME\n",
    "        new_features = new_features.append(feature_dict, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # return concatenation of infestation_history and new_features\n",
    "    return pd.concat([obs, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function in which the data aggregation takes place\n",
    "\n",
    "def data_aggregation2(obs=infestation_history, forestry_districts=districts, raster_dir=r'data_raw/climate_33_months_1000/'):\n",
    "    '''\n",
    "    This function iterates over the rows (observations) of the obs dataframe (in our case infestation_history) and supplements them with the information from the other data sources.\n",
    "    If we do feature engineering that requires meteorological data of a higher time resolution than the observation timeframe it is also done in this dunction.\n",
    "    (for example: new feature that contains the number of days with a maximum temperature below 8 degrees Celsius)\n",
    "    \n",
    "    inputs:\n",
    "        - obs: main dataframe containing observations of (among other things) the target variable \n",
    "        - forestry_districts: dataframe containing the geodata and further information on the forestry districts\n",
    "        - raster_dir: directory where all meteorological raster files are stored\n",
    "        \n",
    "    returns:\n",
    "        - a single dataframe with the aggregated information that can be used for the EDA and modeling process\n",
    "    '''\n",
    "    obs = obs.loc[:1000]\n",
    "    start_time = time.time()\n",
    "    # create an empty dataframe in which we will store our new features\n",
    "    new_features = pd.DataFrame()\n",
    "    \n",
    "    # create a list of all meteorological parameter shorthands that we want to calculate the mean for\n",
    "    parameter_names = [\n",
    "        'TX0', # maximum temperature of the day in degrees Celsius\n",
    "        'TM0', # mean temperature of the day in degrees Celsius\n",
    "        'TN0', # minimum temperature of the day in degrees Celsius\n",
    "        'RF0', # mean relative humidity of the day in %\n",
    "        'SD0', # total sunshine duration of the day in h\n",
    "        'RRU', # total precipitation of the day in mm\n",
    "        'RRK', # corrected total precipitation of the day in mm (corrects systematic errors of the measuring device and installation location such as wetting/evaporation losses)\n",
    "        'FF1', # mean wind velocity of the day 10 metres above ground in m*s-1\n",
    "        'FF2', # mean wind velocity of the day 2 metres above ground in m*s-1\n",
    "        'FFB', # wind speed of the day on the beaufort scale in bft\n",
    "        'RGK', # total global solar irradiation of the day in kWh*m-2\n",
    "        'ETP', # potential evaporation for the day in mm\n",
    "        'GRV', # potential evapotranspiration for the day in mm\n",
    "        'KWU',\n",
    "        'KWK'\n",
    "    ]\n",
    "    \n",
    "    # the obervations from april till september are gathered monthly while they are gathered quarterly from october till march\n",
    "    # create a dictionairy that maps the timeframe values from infestation_history to the pattern that is used in the raster file names \n",
    "    timeframe_dict = {\n",
    "    '01 Januar-März': ['01', '02', '03'],\n",
    "    '04 April': ['04'],\n",
    "    '05 Mai': ['05'],\n",
    "    '06 Juni': ['06'],\n",
    "    '07 Juli': ['07'],\n",
    "    '08 August': ['08'],\n",
    "    '09 September': ['09'],\n",
    "    '10 Oktober-Dezember': ['10', '11', '12']\n",
    "    }\n",
    "    \n",
    "    area_nsne = []\n",
    "    area_nse = []\n",
    "    area_sne = []\n",
    "    area_se = []\n",
    "    centroid_xcoord = []\n",
    "    centroid_ycoord = []\n",
    "    county_name = []\n",
    "    district_name = []\n",
    "    \n",
    "    TX0_mean = []\n",
    "    TM0_mean = []\n",
    "    TN0_mean = []\n",
    "    RF0_mean = []\n",
    "    SD0_mean = []\n",
    "    RRU_mean = []\n",
    "    RRK_mean = []\n",
    "    FF1_mean = []\n",
    "    FF2_mean = []\n",
    "    FFB_mean = []\n",
    "    RGK_mean = []\n",
    "    ETP_mean = []\n",
    "    GRV_mean = []\n",
    "    KWU_mean = []\n",
    "    KWK_mean = []\n",
    "        \n",
    "    # initiate for loop, as we do multiple calculations per row for every row\n",
    "    for current_index, current_obs in obs.iterrows():\n",
    "        \n",
    "        # provide the current progress to user after every 500 rows\n",
    "        if current_index % 200 == 0:\n",
    "            print(f'currently at index {current_index}, elapsed time: {time.time()-start_time}')\n",
    "        \n",
    "        # create a dictionairy in which all features of the current iteration will be collected\n",
    "        feature_dict = {}\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 1-4: AREAS COVERED BY DIFFERENT TYPES OF FOREST\n",
    "        # get respective forest areas from forestry_districts\n",
    "        \n",
    "        # area of non-stateowned, non-endangered forest \n",
    "        area_nsne.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'NSW_SONST'].item() )\n",
    "        # area of non-stateowned, endangered forest \n",
    "        area_nse.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'NSW_FI'].item() )\n",
    "        # area of stateowned, non-endangered forest \n",
    "        area_sne.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'SW_SONST'].item() )\n",
    "        # area of stateowned, endangered forest \n",
    "        area_se.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'SW_FI'].item() )\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 5-6: GEOGRAPHICAL CENTROID COORDINATES\n",
    "        # these two continous variables can maybe used instead of a categorial dummy variable for every forestry district\n",
    "        # already calculated for every district in the geodataframe, we only need make it a feature in our new_features dictionairy\n",
    "        \n",
    "        # get centroid x coordinate \n",
    "        centroid_xcoord.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'centroid_xcoord'].item() )\n",
    "        # get centroid y coordinate \n",
    "        centroid_ycoord.append(forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'centroid_ycoord'].item() )\n",
    "    \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 7-8: FORESTRY DISTRICT AND COUNTY NAMES\n",
    "        # supplement 'REVUFBADR' with the respective county and forestry district name\n",
    "        # these will not be used in model but do enable enable a more descriptive EDA \n",
    "        \n",
    "        # because the difference is almost indistinguishable we simplify our districts by using the 'new' district names for their pre restructuring counterparts\n",
    "        # this makes a qualitative analysis more clear because most of the actual forest in these districts is still the same\n",
    "        \n",
    "        # get leading 0 notation of current 'REVUFBADR' by replacing 9 with 0 for the third digit\n",
    "        current_district = str(current_obs['REVUFBADR'])\n",
    "        query_district = current_district[:2]+current_district[2].replace('9','0')+current_district[3]\n",
    "        #### special case 2198 (Schwarzenberg), does not exist in new structure, so we allocate it to 2101 (Eibenstock)\n",
    "        ####query_district = int(current_district.replace('2108', '2101'))\n",
    "        # special case 2198 (Schwarzenberg), does not exist in new structure, so we leave it\n",
    "        query_district = int(current_district.replace('2108', '2198'))\n",
    "        \n",
    "        # get corresponding county name\n",
    "        county_name.append(forestry_districts.loc[forestry_districts['REVUFBADR'] == query_district, 'KREIS_NAME'].item())\n",
    "        # get corresponding district name\n",
    "        district_name.append( forestry_districts.loc[forestry_districts['REVUFBADR'] == query_district, 'REVUFB_NM'].item())\n",
    "        \n",
    "\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 7-X: MEANS OF THE DIFFERENT METEOROLOGICAL PARAMETERS DURING THE OBSERVATION TIMEFRAME\n",
    "        # even if we later do more sophisticated feature enginnering, the mean for every meteorological parameters will serve as a decent starting point for the analysis\n",
    "        \n",
    "        # the raster_mean() function is already defined, we just need to pass it the specifics of the current observation\n",
    "        # get polygon for current observation from the geodataframe\n",
    "        current_polygon = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'geometry'].item()\n",
    "        # get year for curent observation from obs\n",
    "        current_year = current_obs['Jahr']\n",
    "        # to get the timeframe for the current observation in the right format we use timeframe_dict\n",
    "        current_timeframe = timeframe_dict.get(current_obs['ZR'])\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_TX0_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        TX0_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_TM0_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        TM0_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_TN0_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        TN0_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_RF0_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        RF0_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_SD0_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        SD0_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_RRU_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        RRU_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_RRK_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        RRK_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_FF1_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        FF1_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_FF2_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        FF2_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_FFB_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        FFB_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_RGK_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        RGK_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_ETP_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        ETP_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_GRV_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        GRV_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_KWU_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        KWU_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "        filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_KWK_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "        KWK_mean.append(np.nanmean([raster_mean(filename, current_polygon) for filename in filenames]))\n",
    "        \n",
    "\n",
    "    obs['area_nsne'] = area_nsne\n",
    "    obs['area_nse'] = area_nse\n",
    "    obs['area_sne'] = area_sne\n",
    "    obs['area_se'] = area_se\n",
    "    obs['centroid_xcoord'] = centroid_xcoord\n",
    "    obs['centroid_ycoord'] = centroid_ycoord\n",
    "    obs['county_name'] = county_name \n",
    "    obs['district_name'] = district_name\n",
    "\n",
    "    obs['TX0_mean'] = TX0_mean\n",
    "    obs['TM0_mean'] = TM0_mean\n",
    "    obs['TN0_mean'] = TN0_mean\n",
    "    obs['RF0_mean'] = RF0_mean\n",
    "    obs['SD0_mean'] = SD0_mean\n",
    "    obs['RRU_mean'] = RRU_mean\n",
    "    obs['RRK_mean'] = RRK_mean\n",
    "    obs['FF1_mean'] = FF1_mean\n",
    "    obs['FF2_mean'] = FF2_mean\n",
    "    obs['FFB_mean'] = FFB_mean\n",
    "    obs['RGK_mean'] = RGK_mean\n",
    "    obs['ETP_mean'] = ETP_mean\n",
    "    obs['GRV_mean'] = GRV_mean\n",
    "    obs['KWU_mean'] = KWU_mean\n",
    "    obs['KWK_mean'] = KWK_mean\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # return concatenation of infestation_history and new_features\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function in which the data aggregation takes place\n",
    "\n",
    "def data_aggregation3(obs=infestation_history, forestry_districts=districts, raster_dir=r'data_raw/climate_33_months_1000/'):\n",
    "    '''\n",
    "    This function iterates over the rows (observations) of the obs dataframe (in our case infestation_history) and supplements them with the information from the other data sources.\n",
    "    If we do feature engineering that requires meteorological data of a higher time resolution than the observation timeframe it is also done in this dunction.\n",
    "    (for example: new feature that contains the number of days with a maximum temperature below 8 degrees Celsius)\n",
    "    \n",
    "    inputs:\n",
    "        - obs: main dataframe containing observations of (among other things) the target variable \n",
    "        - forestry_districts: dataframe containing the geodata and further information on the forestry districts\n",
    "        - raster_dir: directory where all meteorological raster files are stored\n",
    "        \n",
    "    returns:\n",
    "        - a single dataframe with the aggregated information that can be used for the EDA and modeling process\n",
    "    '''\n",
    "    obs = obs.loc[:1000]\n",
    "    start_time = time.time()\n",
    "    # create an empty dataframe in which we will store our new features\n",
    "    new_features = pd.DataFrame()\n",
    "    \n",
    "    # create a list of all meteorological parameter shorthands that we want to calculate the mean for\n",
    "    parameter_names = [\n",
    "        'TX0', # maximum temperature of the day in degrees Celsius\n",
    "        'TM0', # mean temperature of the day in degrees Celsius\n",
    "        'TN0', # minimum temperature of the day in degrees Celsius\n",
    "        'RF0', # mean relative humidity of the day in %\n",
    "        'SD0', # total sunshine duration of the day in h\n",
    "        'RRU', # total precipitation of the day in mm\n",
    "        'RRK', # corrected total precipitation of the day in mm (corrects systematic errors of the measuring device and installation location such as wetting/evaporation losses)\n",
    "        'FF1', # mean wind velocity of the day 10 metres above ground in m*s-1\n",
    "        'FF2', # mean wind velocity of the day 2 metres above ground in m*s-1\n",
    "        'FFB', # wind speed of the day on the beaufort scale in bft\n",
    "        'RGK', # total global solar irradiation of the day in kWh*m-2\n",
    "        'ETP', # potential evaporation for the day in mm\n",
    "        'GRV', # potential evapotranspiration for the day in mm\n",
    "        'KWU',\n",
    "        'KWK'\n",
    "    ]\n",
    "    \n",
    "    # the obervations from april till september are gathered monthly while they are gathered quarterly from october till march\n",
    "    # create a dictionairy that maps the timeframe values from infestation_history to the pattern that is used in the raster file names \n",
    "    timeframe_dict = {\n",
    "    '01 Januar-März': ['01', '02', '03'],\n",
    "    '04 April': ['04'],\n",
    "    '05 Mai': ['05'],\n",
    "    '06 Juni': ['06'],\n",
    "    '07 Juli': ['07'],\n",
    "    '08 August': ['08'],\n",
    "    '09 September': ['09'],\n",
    "    '10 Oktober-Dezember': ['10', '11', '12']\n",
    "    }\n",
    "    \n",
    "    obs = pd.merge(obs, forestry_districts[['NSW_FI', 'NSW_SONST', 'SW_FI', 'SW_SONST', 'REVUFBADR', 'centroid_xcoord', 'centroid_ycoord']], on='REVUFBADR')\n",
    "    \n",
    "    # initiate for loop, as we do multiple calculations per row for every row\n",
    "    for current_index, current_obs in obs.iterrows():\n",
    "        \n",
    "        # provide the current progress to user after every 500 rows\n",
    "        if current_index % 200 == 0:\n",
    "            print(f'currently at index {current_index}, elapsed time: {time.time()-start_time}')\n",
    "        \n",
    "        # create a dictionairy in which all features of the current iteration will be collected\n",
    "        feature_dict = {}\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # FEATURES 7-X: MEANS OF THE DIFFERENT METEOROLOGICAL PARAMETERS DURING THE OBSERVATION TIMEFRAME\n",
    "        # even if we later do more sophisticated feature enginnering, the mean for every meteorological parameters will serve as a decent starting point for the analysis\n",
    "        \n",
    "        # the raster_mean() function is already defined, we just need to pass it the specifics of the current observation\n",
    "        # get polygon for current observation from the geodataframe\n",
    "        current_polygon = forestry_districts.loc[forestry_districts['REVUFBADR'] == current_obs['REVUFBADR'], 'geometry'].item()\n",
    "        # get year for curent observation from obs\n",
    "        current_year = current_obs['Jahr']\n",
    "        # to get the timeframe for the current observation in the right format we use timeframe_dict\n",
    "        current_timeframe = timeframe_dict.get(current_obs['ZR'])\n",
    "        \n",
    "        # raster_mean() function call for every meteorological parameter we want to use\n",
    "        for current_parameter in parameter_names:\n",
    "            # construct the right filename(s) for the current observation\n",
    "            filenames = [fr'{raster_dir}GRID_1_Messungen_Tageswerte_2020_{current_parameter}_MW_{current_year}{current_month}00_utm.asc' for current_month in current_timeframe]\n",
    "            \n",
    "            # function call and assignment to feature_dict\n",
    "            feature_dict[f'{current_parameter}_mean'] = np.nanmean([raster_mean(filename, current_polygon) for filename in filenames])\n",
    "\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        # STORE ALL FEATURES OF CURRENT OBSERVATION IN DATAFRAME\n",
    "        new_features = new_features.append(feature_dict, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # return concatenation of infestation_history and new_features\n",
    "    return pd.concat([obs, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make column with district names (connected) and give them IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_districts(district):\n",
    "    district = str(district)\n",
    "    return int(\n",
    "        (\n",
    "            district[:2] + \n",
    "            district[2].replace('9','0') + \n",
    "            district[3]\n",
    "        ).replace('2108', '2198') # special case 2198 (Schwarzenberg), does not exist in new structure, so we leave it as is\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'REVUFBADR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\python377\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2890\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'REVUFBADR'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-3e3471090a67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minfestation_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfestation_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'REVUFBADR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconnect_districts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minfestation_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfestation_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistricts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'KREIS_NAME'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'REVUFB_NM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'REVUFBADR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'REVUFBADR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_drop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minfestation_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'REVUFBADR_drop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python377\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python377\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2891\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'REVUFBADR'"
     ]
    }
   ],
   "source": [
    "infestation_history['query'] = infestation_history['REVUFBADR'].apply(lambda x: connect_districts(x))\n",
    "infestation_history = pd.merge(infestation_history, districts[['KREIS_NAME', 'REVUFB_NM', 'REVUFBADR']], left_on='query', right_on='REVUFBADR', suffixes=('','_drop'))\n",
    "infestation_history.drop(['query', 'REVUFBADR_drop'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently at index 0, elapsed time: 0.0\n",
      "currently at index 200, elapsed time: 64.26541352272034\n",
      "currently at index 400, elapsed time: 144.32509183883667\n",
      "currently at index 600, elapsed time: 209.92694115638733\n",
      "currently at index 800, elapsed time: 279.85026502609253\n",
      "currently at index 1000, elapsed time: 356.805823802948\n"
     ]
    }
   ],
   "source": [
    "new = data_aggregation2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently at index 0, elapsed time: 0.0\n",
      "currently at index 200, elapsed time: 59.567248582839966\n",
      "currently at index 400, elapsed time: 136.22443866729736\n",
      "currently at index 600, elapsed time: 202.53974676132202\n",
      "currently at index 800, elapsed time: 274.6814339160919\n",
      "currently at index 1000, elapsed time: 353.6233859062195\n"
     ]
    }
   ],
   "source": [
    "new = data_aggregation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently at index 0, elapsed time: 0.015145540237426758\n",
      "currently at index 200, elapsed time: 56.058919191360474\n",
      "currently at index 400, elapsed time: 131.77184462547302\n",
      "currently at index 600, elapsed time: 197.81083941459656\n",
      "currently at index 800, elapsed time: 268.4805302619934\n",
      "currently at index 1000, elapsed time: 344.97144532203674\n"
     ]
    }
   ],
   "source": [
    "new = data_aggregation3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barkbeetle_dataset.to_csv('barkbeetle_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barkbeetle_dataset = pd.read_csv('barkbeetle_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12637 entries, 0 to 12636\n",
      "Data columns (total 29 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   REVUFBADR        12637 non-null  int64  \n",
      " 1   Jahr             12637 non-null  int64  \n",
      " 2   ZR               12637 non-null  object \n",
      " 3   Eigentumsgruppe  12637 non-null  object \n",
      " 4   Zugang           12637 non-null  float64\n",
      " 5   Abgang           12637 non-null  float64\n",
      " 6   ETP_mean         12000 non-null  float64\n",
      " 7   FF1_mean         12000 non-null  float64\n",
      " 8   FF2_mean         12000 non-null  float64\n",
      " 9   FFB_mean         12000 non-null  float64\n",
      " 10  GRV_mean         12000 non-null  float64\n",
      " 11  KWK_mean         12000 non-null  float64\n",
      " 12  KWU_mean         12000 non-null  float64\n",
      " 13  RF0_mean         12000 non-null  float64\n",
      " 14  RGK_mean         12000 non-null  float64\n",
      " 15  RRK_mean         12000 non-null  float64\n",
      " 16  RRU_mean         12000 non-null  float64\n",
      " 17  SD0_mean         12000 non-null  float64\n",
      " 18  TM0_mean         12000 non-null  float64\n",
      " 19  TN0_mean         12000 non-null  float64\n",
      " 20  TX0_mean         12000 non-null  float64\n",
      " 21  area_nse         12637 non-null  float64\n",
      " 22  area_nsne        12637 non-null  float64\n",
      " 23  area_se          12637 non-null  float64\n",
      " 24  area_sne         12637 non-null  float64\n",
      " 25  centroid_xcoord  12637 non-null  float64\n",
      " 26  centroid_ycoord  12637 non-null  float64\n",
      " 27  county_name      12637 non-null  object \n",
      " 28  district_name    12637 non-null  object \n",
      "dtypes: float64(23), int64(2), object(4)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "barkbeetle_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barkbeetle_dataset.dropna(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
